<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="this is amazing"><title>XGBoost 笔记 | Matafight's world</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">XGBoost 笔记</h1><a id="logo" href="/.">Matafight's world</a><p class="description">点滴进步</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">XGBoost 笔记</h1><div class="post-meta">Mar 14, 2017<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2017/03/14/XGBoost-简介/" href="/2017/03/14/XGBoost-简介/#comments" class="ds-thread-count"></a><div class="post-content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  XGBoost是一种基于决策树（CART）的分布式的高效的梯度提升算法，它可被应用到分类、回归、排序等任务中，与一般的GBDT算法相比，XGBoost主要有以下几个优点：</p>
<ol>
<li>对叶节点的权重进行了惩罚，相当于添加了正则项，防止过拟合</li>
<li>XGBoost的目标函数优化利用了损失函数关于待求函数的二阶导数，而GBDT只利用了一阶信息</li>
<li>XGBoost支持列采样，类似于随机森林，构建每棵树时对属性进行采样，训练速度快，效果好</li>
<li>类似于学习率，学习到一棵树后，对其权重进行缩减，从而降低该棵树的作用，提升可学习空间</li>
<li>构建树的算法包括精确的算法和近似的算法，近似的算法对每维特征加权分位进行分桶，具体的算法利用到了损失函数关于待求树的二阶导数。</li>
<li>添加了对于稀疏数据的支持，当数据的某个特征缺失时，将该数据划分到默认的子节点，本文提出了一个算法来求解这个默认方向。</li>
<li>可并行的近似直方图算法，分裂节点时，数据在block中按列存放，而且已经经过了预排序，因此可以并行计算，即同时对各个属性遍历最优分裂点</li>
</ol>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>  boosting是属于串行的集成方法，其预测函数为多个基分类器的集成，其学习过程也是先学习前(t-1)个基分类器，再学习第t个基分类器。XGBoost中最主要的基学习器为CART（分类与回归树），因此，其预测函数为:<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_1.PNG" alt=""><br>  其中$K$表示有$K$个决策树<img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_2.PNG" alt="">表示决策树空间，$q(x)$表示将输入样本$x$映射到树的叶子节点$q(x)$，其对应叶子节点的标号为$\omega_{q(x)}$。<br>  因此，正则化的目标函数可以写成：<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_3.PNG" alt=""><br>  上述目标函数的参数是决策树，因此不能在一般的欧式空间优化，这里类似于adaboost算法的优化过程，也是利用加性模型的特点来训练。假设前$t-1$步迭代优化得到的模型为$f^{t-1}(x)$,在第$t$步中，待求参数为$f^t$，则第$t$步的目标函数为:<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_4.PNG" alt=""><br>将上式进行二阶泰勒展开可以得到：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_5.PNG" alt=""><br>其中<img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_6.PNG" alt=""><br>去掉与待求参数无关的常数项，从而得到新的优化目标为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_7.PNG" alt=""><br>将上式变形，将关于样本迭代转换为关于树的叶子节点迭代：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_8.PNG" alt=""></p>
<p>这样，对于给定的树结构，可以直接计算得到叶子节点的标号：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_9.PNG" alt=""><br>以及相对应的最优目标函数值：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_10.PNG" alt=""></p>
<p>但是,由于树的结构是未知的，而且也不可能去遍历所有的树结构。因此，本文采用贪婪算法来分裂节点，从根节点开始，遍历所有属性，遍历属性的可能取值，记分到左子树的样本集为$I_L$,分到右子树的样本集为$I_R$，则分裂该节点导致的损失减少值为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_11.PNG" alt=""><br>我们希望找到一个属性以及其对应的大小，使得上式取值最大。</p>
<h2 id="分裂节点算法-SPLIT-FINDING-ALGORITHMS"><a href="#分裂节点算法-SPLIT-FINDING-ALGORITHMS" class="headerlink" title="分裂节点算法(SPLIT FINDING ALGORITHMS)"></a>分裂节点算法(SPLIT FINDING ALGORITHMS)</h2><p>因为树结构未知，只能采用贪婪的算法，从根节点出发，每次选择一个属性及其对应的值，使得损失函数减少最多，根据选择的属性分裂节点。论文中给出了精确的和近似的算法，当数据量非常大时，采用近似的算法可以有效减少计算量。</p>
<h3 id="精确贪婪算法-Basic-Exact-Greedy-Algorithms"><a href="#精确贪婪算法-Basic-Exact-Greedy-Algorithms" class="headerlink" title="精确贪婪算法(Basic Exact Greedy Algorithms)"></a>精确贪婪算法(Basic Exact Greedy Algorithms)</h3><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_12.PNG" alt=""><br>这里的$m$值通常小于样本维度$d$,表示列采样得到的属性个数，值得注意的是，由于要遍历所有的属性的所有取值，因此，通常需要在训练之前对所有样本做一个预排序(pre-sort)，从而避免每次选择属性都要重新排序。</p>
<h3 id="近似算法-Approximate-Algorithm-for-Split-Finding"><a href="#近似算法-Approximate-Algorithm-for-Split-Finding" class="headerlink" title="近似算法(Approximate Algorithm for Split Finding)"></a>近似算法(Approximate Algorithm for Split Finding)</h3><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_13.PNG" alt=""><br>对于值为连续值的特征，当样本数非常大时，该特征取值过多，遍历所有取值复杂度较高，而且容易过拟合。因此，考虑将特征值分桶，即找到$l$个分位点，将位于相邻分位点之间的样本分在一个桶中，在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。注意到上面算法流程中说明了有全局的近似(global)和局部(local)的近似，所谓全局就是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部就是在具体的某一次分裂节点的过程中采用近似算法。</p>
<h3 id="Weighted-Quantile-Sketch"><a href="#Weighted-Quantile-Sketch" class="headerlink" title="Weighted Quantile Sketch"></a>Weighted Quantile Sketch</h3><p><strong>带权重直方图算法</strong><br>主要用于近似算法中分位点的计算，假设分位点为 ${s<em>{k1},s</em>{k2},..,s<em>{kl}}$，假设 $(x</em>{1k},h<em>1),(x</em>{2k},h<em>2),..,(x</em>{nk},h_n)$ 表示所有样本的第 $k$ 个特征值及二阶导数，定义一个rank function 如下：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_14.PNG" alt=""><br>希望得到的分位点满足如下条件：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_15.PNG" alt=""><br>这意味着大概有$\frac{1}{\epsilon}$个分位点。</p>
<p><strong>为什么用二阶导数作为权重？</strong><br><em>因为目标函数还可以写成带权的形式：</em><br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_16.PNG" alt=""><br>上式相当于带权重的损失函数，权重为$h_i$。<br>为了优化该问题，本文还提出了分布式 <em>weighted quantile sketch algorithm</em> ，该算法的优点是解决了带权重的直方图算法问题，以及有理论保证。</p>
<h3 id="Sparsity-aware-Split-Finding"><a href="#Sparsity-aware-Split-Finding" class="headerlink" title="Sparsity-aware Split Finding"></a>Sparsity-aware Split Finding</h3><p>XGBoost还特别设计了针对稀疏数据的算法，假设样本的第$i$个特征缺失时，无法利用该特征对样本进行划分，这里的做法是将该样本默认地分到指定的子节点，至于具体地分到哪个节点还需要下面的算法来计算：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_17.PNG" alt=""><br>该算法的主要思想是，分别假设特征缺失的样本属于右子树和左子树，而且只在不缺失的样本上迭代，分别计算缺失样本属于右子树和左子树的增益，选择增益最大的方向为缺失数据的默认方向。</p>
<h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="Column-Block-for-Parallel-Learning"><a href="#Column-Block-for-Parallel-Learning" class="headerlink" title="Column Block for Parallel Learning"></a>Column Block for Parallel Learning</h3><p>算法中最耗时的部分就是预排序，为了节省排序的时间，XGBoost将数据存在内存单元block中(??)，同时在block采用CSC 格式存放(Compressed Column format)，每一列(一个属性列)均升序存放，这样，一次读入数据并排好序后，以后均可使用。在精确贪心算法中，将所有数据均导入内存，算法只要在数据中线性扫描已经预排序过的特征就可以。对于近似算法，可以用多个block(Multiple blocks)分别存储不同的样本集，多个block可以并行计算，<br>重要的是，由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行split finding算法，从而并行化split finding.</p>
<h3 id="Cache-aware-Access"><a href="#Cache-aware-Access" class="headerlink" title="Cache-aware Access"></a>Cache-aware Access</h3><p>由于样本按特征进行了预排序，样本对应的统计量(一阶和二阶梯度)需要根据行索引来查找，这导致了内存的不连续访问，容易导致cpu cache命中率降低。—这块没太搞懂</p>
<h3 id="Blocks-for-Out-of-core-Computation"><a href="#Blocks-for-Out-of-core-Computation" class="headerlink" title="Blocks for Out-of-core Computation"></a>Blocks for Out-of-core Computation</h3><p>为了更好地利用计算机的磁盘空间，对于不能一次性导入到内存的数据，我们将数据分成多个block存在磁盘上，在计算过程中，用另外的线程读取数据，但是由于磁盘IO速度太慢，通常更不上计算的速度。所以，我们采用了下面两种方法有优化速度和存储：</p>
<ol>
<li>Block compression<br>将block按列压缩，对于行索引，只保存第一个索引值，然后只保存该数据与第一个索引值之差(offset)，一共用16个bits来保存 offset,因此，一个block一般有$2^{16}$个样本。</li>
<li>Block sharding<br>提高磁盘的吞吐量(??)</li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matafight.github.io/2017/03/14/XGBoost-简介/" data-id="cjtmefp94000dlg9wxvzhjl0o" class="article-share-link">分享到</a><div class="tags"><a href="/tags/XGBoost/">XGBoost</a></div><div class="post-nav"><a href="/2017/03/23/xgboost 常用参数/" class="pre">xgboost 常用参数</a><a href="/2017/02/21/C-泛型算法/" class="next">C++ primer-泛型算法</a></div><div data-thread-key="2017/03/14/XGBoost-简介/" data-title="XGBoost 笔记" data-url="http://matafight.github.io/2017/03/14/XGBoost-简介/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2017/03/14/XGBoost-简介/" data-title="XGBoost 笔记" data-url="http://matafight.github.io/2017/03/14/XGBoost-简介/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://matafight.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sgd/">sgd</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim/">vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客本身/">博客本身</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记-Matlab/">杂记|Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知乎/">知乎</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/XGBoost/" style="font-size: 15px;">XGBoost</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a> <a href="/tags/Model-Selection/" style="font-size: 15px;">Model-Selection</a> <a href="/tags/Cython/" style="font-size: 15px;">Cython</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/kaggle/" style="font-size: 15px;">kaggle</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/lightgbm/" style="font-size: 15px;">lightgbm</a> <a href="/tags/latex/" style="font-size: 15px;">latex</a> <a href="/tags/normalization/" style="font-size: 15px;">normalization</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/编码/" style="font-size: 15px;">编码</a> <a href="/tags/毕设/" style="font-size: 15px;">毕设</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/工具/" style="font-size: 15px;">工具</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/FAQ/" style="font-size: 15px;">FAQ</a> <a href="/tags/好的博客/" style="font-size: 15px;">好的博客</a> <a href="/tags/求职/" style="font-size: 15px;">求职</a> <a href="/tags/machien-learning/" style="font-size: 15px;">machien learning</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/sgd/" style="font-size: 15px;">sgd</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/lightgbm-相关参数/">lightgbm 相关参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/12/git-note/">git note</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/Deep-Learning-Chapter16/">Deep Learning Chapter16</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/CNN结构的发展/">CNN结构的发展</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/25/Deep-learning-chapter10-part-1/">Deep learning chapter10</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/17/广义线性模型/">广义线性模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/29/gbdt-xgboost与lightgbm/">gbdt,xgboost与lightgbm</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/kaggle竞赛总结/">kaggle竞赛总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/python中的编码问题/">python中的编码问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/24/kaggle经验/">kaggle经验</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://tsien.github.io/" title="tsien" target="_blank">tsien</a><ul></ul><a href="http://www.ruanyifeng.com/blog/" title="阮一峰的网络日志" target="_blank">阮一峰的网络日志</a><ul></ul><a href="http://blog.csdn.net/abcjennifer/article/category/1226975" title="Rachel Zhang的专栏" target="_blank">Rachel Zhang的专栏</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matafight's world.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'matafight1994'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>