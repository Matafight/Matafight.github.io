<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="this is amazing"><title>kaggle经验 | Matafight's world</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">kaggle经验</h1><a id="logo" href="/.">Matafight's world</a><p class="description">点滴进步</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">kaggle经验</h1><div class="post-meta">Jun 24, 2017<span> | </span><span class="category"><a href="/categories/kaggle/">kaggle</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2017/06/24/kaggle经验/" href="/2017/06/24/kaggle经验/#comments" class="ds-thread-count"></a><div class="post-content"><p>最近参加了kaggle的一个比赛，看了好多关于数据处理，特征分析以及集成学习的一些内容，这些都是数据挖掘大神总结出来的经验，在这里总结一下我的收获，以及谈一下我自己的一些想法。</p>
<p>参加数据挖掘比赛一般要从以下几个方面着手：</p>
<h3 id="数据探索与预处理"><a href="#数据探索与预处理" class="headerlink" title="数据探索与预处理"></a>数据探索与预处理</h3><p>所谓数据探索就是利用各种图表直观地观察各种分布情况，从而为接下来的数据预处理提供一个预处理的方向。</p>
<ol>
<li><p>首先分析数据，统计各个类型特征的个数，以及统计缺失值情况</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_train = pd.read_csv(<span class="string">'../input/train.csv'</span>)</span><br><span class="line"><span class="comment"># distinguish numerical attributes and object attributes and target column</span></span><br><span class="line">dtype_df = df_train.dtypes.reset_index()</span><br><span class="line">dtype_df.columns = [<span class="string">"Count"</span>, <span class="string">"Column Type"</span>]</span><br><span class="line">summary_dtypes = dtype_df.groupby(<span class="string">"Column Type"</span>).aggregate(<span class="string">'count'</span>).reset_index()</span><br><span class="line">print(summary_dtypes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#样例输出</span></span><br><span class="line"><span class="comment">#  Column Type  Count</span></span><br><span class="line"><span class="comment">#0       int64    369</span></span><br><span class="line"><span class="comment">#1     float64      1</span></span><br><span class="line"><span class="comment">#2      object      8</span></span><br><span class="line"></span><br><span class="line">target = [<span class="string">'y'</span>]</span><br><span class="line">columns = df_train.columns</span><br><span class="line">num_feats = df_train.dtypes.index[df_train.dtypes!=<span class="string">'object'</span>]</span><br><span class="line">cate_feats = df_train.dtypes.index[df_train.dtypes==<span class="string">'object'</span>]</span><br><span class="line">num_feats = list(set(num_feats)-set(target))</span><br><span class="line"><span class="comment">#输出所有数值型特征和离散型特征</span></span><br><span class="line">print(<span class="string">'numerical features:'</span>)</span><br><span class="line">print(num_feats)</span><br><span class="line">print(<span class="string">'categorical features:'</span>)</span><br><span class="line">print(cate_feats)</span><br><span class="line">print(<span class="string">'target column:'</span>)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#统计缺失值</span></span><br><span class="line"><span class="comment"># check if there are any missing values</span></span><br><span class="line">print(<span class="string">'the following numerical features contains null value'</span>)</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> num_feats:</span><br><span class="line">    <span class="keyword">if</span>(np.sum(df_train[column].isnull())!=<span class="number">0</span>):</span><br><span class="line">        print(column)</span><br><span class="line">print(<span class="string">'the following categorical features contains null value'</span>)</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> cate_feats:</span><br><span class="line">    <span class="keyword">if</span>(np.sum(df_train[column].isnull()!=<span class="number">0</span>)):</span><br><span class="line">        print(column)</span><br><span class="line"><span class="comment"># fill the missing values with mean or median , all depends on the distribution of the corresponding variable</span></span><br><span class="line">missing_df = df_train.isnull().sum(axis=<span class="number">0</span>).reset_index()</span><br><span class="line">missing_df.columns = [<span class="string">'column_name'</span>, <span class="string">'missing_count'</span>]</span><br><span class="line">missing_df = missing_df.ix[missing_df[<span class="string">'missing_count'</span>]&gt;<span class="number">0</span>]</span><br><span class="line">missing_df = missing_df.sort_values(by=<span class="string">'missing_count'</span>)</span><br><span class="line">missing_df</span><br></pre></td></tr></table></figure>
</li>
<li><p>目标变量的分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.figure(figsize = (<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">tar_val = np.sort(df_train[target].values)</span><br><span class="line">sns.distplot(tar_val,bins=<span class="number">50</span>,kde=<span class="keyword">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'target value'</span>,fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<p>如果目标变量为连续值且其值域范围较大，可以考虑对其做对数变换，并用变换后的值建模。<br>如果目标变量为离散值且正负样本不均衡，可以考虑下采样或这上采样，可以采用对不平衡数据友好的auc等指标指导模型选择，而且再划分训练集和验证集的使用要注意各个类的样本再训练集或验证集中要分布均匀，可以使用sklearn中的StratifiedKFold函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">kf = StratifiedKFold(n_splits = <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> kf.split(train_X):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>各个特征变量的分布</p>
<p>   特征变量为连续值：如果为长尾分布并且考虑使用线性模型，需要对变量进行对数变换</p>
<p>   特征变量为离散值：观察每个离散值的频率分布，对出现频率较低的变量统一归为“其他类”，之后进行one-hot 编码或采用数值编码<br>   数值编码可以采用LabelEncoder，one-hot encoder也有对应的函数</p>
<p>   如何处理缺失值？<br>   特征为连续值：如果分布为偏正态分布，可以用均值填充，如果为长尾分布，一般用中值，避免受outlier的影响。<br>   特征为离散值：用众数填充。</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">lbl_enc = LabelEncoder()</span><br><span class="line">lbl_enc.fit(list(df_train[column_name].values))</span><br><span class="line">df_train[column_name] = lbl_enc.transform(list(df_train[column_name].values))</span><br><span class="line"><span class="comment"># 貌似如果要使用OneHotEncoder函数需要保证column的值是数值，所以需要先用labelencoder预处理一下</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">ohe = OneHotEncoder()</span><br><span class="line">ohe.fit(df_train[column_name])</span><br><span class="line">ohe_features = ohe.transform(df_train[column_name])</span><br></pre></td></tr></table></figure>
<p>   我一般使用pandas内置的函数做one-hot编码：</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...one hot encoding of categorical variables</span></span><br><span class="line">categorical =  [<span class="string">"X0"</span>, <span class="string">"X1"</span>, <span class="string">"X2"</span>, <span class="string">"X3"</span>, <span class="string">"X4"</span>, <span class="string">"X5"</span>, <span class="string">"X6"</span>, <span class="string">"X8"</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> categorical:</span><br><span class="line">    dummies = pd.get_dummies(all_df[f], prefix = f, prefix_sep = <span class="string">'_'</span>)</span><br><span class="line">    all_df = pd.concat([all_df, dummies], axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>各个特征与标号地分布情况<br>对于categorical features 一般采用box-plot分析，对于数值型特征一般采用scatter或者直接plot<br>box-plot 代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var_name = <span class="string">"X6"</span></span><br><span class="line">col_order = np.sort(df_train[var_name].unique()).tolist()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">sns.boxplot(x=var_name, y=<span class="string">'y'</span>, data=df_train, order=col_order)</span><br><span class="line">plt.xlabel(var_name, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">"Distribution of y variable with "</span>+var_name, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>特征两两之间的分布与相关度<br>可以有助于发现高相关和共线性的特征</p>
</li>
</ol>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><ol>
<li>合并文件</li>
<li>缺失值处理</li>
<li>离群点处理</li>
<li>特征编码</li>
</ol>
<p>这个部分其实基本上在第一部分数据探索与预处理中已经提到了，主要是因为数据探索主要也是探索需要处理的特征，所以数据清洗与数据探索与预处理是有很大一部分耦合的。</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>坊间传言，特征决定了效果的上限，而算法只是再不断地逼近这个上限，这充分说明了特征在数据挖掘比赛中的重要性。<br>特征工程按照我自己的理解主要是与领域知识相关的，我觉得可以把它分为三个部分：</p>
<ol>
<li>特征构建<br>特征构建就是根据原始的数据构建出可以用来训练模型的数据，这主要与参赛者的领域知识密切相关，比如在自然语言处理中常用的tf-idf特征，参赛者对相关领域的了解程度直接决定了构建的特征好与坏。</li>
<li><p>特征抽取<br>一般是指用PCA,LDA,ICA,SVD等方法自动抽取出有物理意义的特征，这些抽取出的特征一般都是直接与原始特征stacking到一起，而不是要舍弃原始特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_component = <span class="number">12</span>)</span><br><span class="line">pca.fit(df_train)</span><br><span class="line">pca_comp = pca.transform(df_train)</span><br></pre></td></tr></table></figure>
</li>
<li><p>特征选择<br>Lasso，ElasticNet等模型能起到特征选择的作用，只保留重要的特征，而基于决策树的一些方法如RandomForest,xgboost等也能够根据构建决策树的过程中各个特征的贡献度给出各个特征的打分，可以根据打分判断各个特征的重要性。当然，也可以使用卡方检验来对特征的重要度排序。根据特征的重要程度选择相对有用的特征用于建模。</p>
</li>
</ol>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>对于稀疏型数据（如文本特征，one-hot的ID特征），一般采用线性模型。 Random Forest 和 GBDT等树模型不太适用于稀疏的特征，但可以先对特征降维(PCA,LDA,SVD/LSA)，再使用这些特征。稀疏数据直接输入DNN会导致网络weight过多，不利于优化， 也可以考虑先降维，或者对ID类特征只用embedding的方式。</p>
<p>对于稠密型特征，一般使用xgboost建模</p>
<p>数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏数据进行建模，将其输出与稠密特征一起再输入到XGboost等模型中。</p>
<p>如果要使用线性模型，一般要先标准化输入特征，因为各个特征的尺度可能不一样，标准化能归一化到同一尺度，从而避免某个特征占主导地位，而且对于采用梯度下降优化算法的算法还能加速收敛。</p>
<p>kaggle大神Abhishek在他的博客中给出了常用的数据挖掘算法并给出了各个算法需要调整的参数及其范围：</p>
<p>Classification:</p>
<p>Random Forest，<br>GBM，<br>Logistic Regression，<br>Naive Bayes，<br>Support Vector Machines，<br>k-Nearest Neighbors</p>
<p>Regression：</p>
<p>Random Forest，<br>GBM，<br>Linear Regression，<br>Ridge，<br>Lasso，<br>SVR</p>
<p><img src="http://7xiegr.com1.z0.glb.clouddn.com/AAEAAQAAAAAAAAilAAAAJDhiNGE5YTVmLTJhNTAtNGNkZS1hNDAwLTY5YTJiMTE1ZmI3Zg.png" alt=""></p>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><ol>
<li>Voting 和 Averaging<br>直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。</li>
<li>Stacking<br>这里重点介绍stacking</li>
</ol>
<p><img src="http://7xiegr.com1.z0.glb.clouddn.com/stacking.jpg" alt=""></p>
<p>stacking与bagging和boosting一样，都属于集成学习的一种，bagging是一种并行的集成学习模式，boosting是一种串行的集成学习模式，而stacking则不同于这两种方法，bagging和boosting都是只有一层的集成学习，而stacking则可以有多层，stacking通过将前一层的输出输入到下一层中，从而组成了一种多层模型的集成学习模式。如上图所示，假设现在要训练第一层模型，具体步骤如下：</p>
<pre><code>1. 首先将训练集分成5-folds
2. 在其中4-folds上训练，给出在剩下的一个fold 上的预测值，同时给出测试集的预测值。
3. 重复步骤2，5次。
4. 将步骤2中5次的验证集上的预测值组合成一个长的向量，长度为训练集的大小，这个向量就是该算法输入到下一层的输入，同时对步骤2中五次的测试集预测值求平均
5. 换另一个算法重复步骤1到步骤4
6. 将所有算法得到的训练集预测值按列组合成一个高为训练集大小，宽度为算法个数的矩阵，该矩阵就是输入到下一层的样本，每一行为一个样本，每一列为一个特征，同时对于测试集上的预测值也组合成一个矩阵，输入到下一层中当测试集。
（貌似挺拗口）
</code></pre><p>上面是一种stacking的方法，可以看出每层算法的个数决定了下一层输入样本的特征数，所以算法少的话，下一层的输入特征就变得少了，所以stacking还有其他的变种，其中一种是将上一层的输入与上一层的输出直接按列组合成一个新的矩阵输入到下一层中，比如现在有输入样本 100$\times$300,如果第一层只有一个算法，那么输入到第二层的样本矩阵大小为  100$\times$301，有效避免了特征过少的问题。 结合我们之前所说的，对于稀疏特征常采用线性模型，对于稠密特征采用复杂一点的模型比如xgboost,如果既有稀疏特征又有稠密特征，可以在第一层先使用线性模型对稀疏特征建模，将其输出在第二层与稠密特征组合形成第二层的输入，并在第二层采用xgboost等模型。</p>
<h3 id="自动化框架"><a href="#自动化框架" class="headerlink" title="自动化框架"></a>自动化框架</h3><p>之前我也曾试过kaggle上的一些入门竞赛，但是没有那么认真做，写完代码，提交完结果之后就没怎么关注过了，所以我的代码没有系统性，也没有做好整理，所以，这次参加比赛的代码又从头开始写了。 。从上面的介绍可以看出，写好一套代码，是可以在多个数据集上重用的，而且，在做特征工程的时候，常常需要不时地添加或删除特征，所以输入数据也在一直变化，如果能够将后面地几个步骤如模型选择，参数调优，模型融合写成一个自动化地框架，那就非常方便了，因为不用每次都要手动调节参数，手动记录结果，大大地节省了时间也提高了效率。目前我主要参考的是kaggle大神 Chenglong Chen在 <a href="https://github.com/Matafight/Kaggle_CrowdFlower" target="_blank" rel="noopener">Crowdflower</a>竞赛中开源的代码，他的代码在模型融合中使用了bagged ensemble selection, 来自于一篇ICML2004的论文<a href="https://scholar.google.com.hk/scholar?q=Ensemble+selection+from+libraries+of+models&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Ensemble<br>selection from libraries of models</a>。另一个同样来自Chenglong Chen的开源代码<a href="https://github.com/Matafight/Kaggle_HomeDepot" target="_blank" rel="noopener">HomeDepot</a>中采用了stacking的模型融合方式，看来单一的模型融合方式不能适用于多种任务，还是要针对具体任务具体分析，多尝试，从而得到相对好的结果。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur" target="_blank" rel="noopener">https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur</a><br><a href="https://zhuanlan.zhihu.com/p/26820998" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26820998</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matafight.github.io/2017/06/24/kaggle经验/" data-id="cjx65kplz001cl89wniwyip3k" class="article-share-link">分享到</a><div class="tags"><a href="/tags/kaggle/">kaggle</a></div><div class="post-nav"><a href="/2017/06/30/python中的编码问题/" class="pre">python中的编码问题</a><a href="/2017/05/12/cython-introduction-md/" class="next">Cython简单介绍</a></div><div data-thread-key="2017/06/24/kaggle经验/" data-title="kaggle经验" data-url="http://matafight.github.io/2017/06/24/kaggle经验/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2017/06/24/kaggle经验/" data-title="kaggle经验" data-url="http://matafight.github.io/2017/06/24/kaggle经验/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://matafight.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sgd/">sgd</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim/">vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客本身/">博客本身</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记-Matlab/">杂记|Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知乎/">知乎</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/XGBoost/" style="font-size: 15px;">XGBoost</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a> <a href="/tags/Cython/" style="font-size: 15px;">Cython</a> <a href="/tags/Model-Selection/" style="font-size: 15px;">Model-Selection</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/kaggle/" style="font-size: 15px;">kaggle</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/lightgbm/" style="font-size: 15px;">lightgbm</a> <a href="/tags/latex/" style="font-size: 15px;">latex</a> <a href="/tags/normalization/" style="font-size: 15px;">normalization</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/编码/" style="font-size: 15px;">编码</a> <a href="/tags/毕设/" style="font-size: 15px;">毕设</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/工具/" style="font-size: 15px;">工具</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/FAQ/" style="font-size: 15px;">FAQ</a> <a href="/tags/求职/" style="font-size: 15px;">求职</a> <a href="/tags/好的博客/" style="font-size: 15px;">好的博客</a> <a href="/tags/machien-learning/" style="font-size: 15px;">machien learning</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/sgd/" style="font-size: 15px;">sgd</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/21/理解word2vec/">理解word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/lightgbm-相关参数/">lightgbm 相关参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/12/git-note/">git note</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/Deep-Learning-Chapter16/">Deep Learning Chapter16</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/CNN结构的发展/">CNN结构的发展</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/25/Deep-learning-chapter10-part-1/">Deep learning chapter10</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/17/广义线性模型/">广义线性模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/29/gbdt-xgboost与lightgbm/">gbdt,xgboost与lightgbm</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/kaggle竞赛总结/">kaggle竞赛总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/python中的编码问题/">python中的编码问题</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://tsien.github.io/" title="tsien" target="_blank">tsien</a><ul></ul><a href="http://www.ruanyifeng.com/blog/" title="阮一峰的网络日志" target="_blank">阮一峰的网络日志</a><ul></ul><a href="http://blog.csdn.net/abcjennifer/article/category/1226975" title="Rachel Zhang的专栏" target="_blank">Rachel Zhang的专栏</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matafight's world.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'matafight1994'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>