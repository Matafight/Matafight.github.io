<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matafight&#39;s world</title>
  
  <subtitle>点滴进步</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matafight.github.io/"/>
  <updated>2018-01-18T10:46:11.261Z</updated>
  <id>http://matafight.github.io/</id>
  
  <author>
    <name>guo_sc</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用gluon写rnn</title>
    <link href="http://matafight.github.io/2018/01/18/%E7%94%A8gluon%E5%86%99rnn/"/>
    <id>http://matafight.github.io/2018/01/18/用gluon写rnn/</id>
    <published>2018-01-18T09:16:13.000Z</published>
    <updated>2018-01-18T10:46:11.261Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Deel learning" scheme="http://matafight.github.io/categories/Deel-learning/"/>
    
    
      <category term="gluon" scheme="http://matafight.github.io/tags/gluon/"/>
    
  </entry>
  
  <entry>
    <title>新的一年</title>
    <link href="http://matafight.github.io/2018/01/15/%E6%96%B0%E7%9A%84%E4%B8%80%E5%B9%B4/"/>
    <id>http://matafight.github.io/2018/01/15/新的一年/</id>
    <published>2018-01-15T05:41:15.000Z</published>
    <updated>2018-01-17T06:27:22.988Z</updated>
    
    <content type="html"><![CDATA[<h2 id="我的2017-忙碌"><a href="#我的2017-忙碌" class="headerlink" title="我的2017-忙碌"></a>我的2017-忙碌</h2><p>2017是非常忙碌的一年，刚过完年就投入了找实习的大军中，好不容易找到了一个实习，过了不久就重新投入到了秋招的大军中。感觉自己不是在去笔试面试的路上，就是在准备面试和笔试，但我觉得这种准备其实是非常浪费时间的。回过头来，这一年自己的技术成长很有限，在找工作的时候一直在复习和巩固基础知识以应对面试。只有在工作定下来的时候才静下心来研究对技术成长有益的东西。2017年主要在看两本书:Ian Goodfellow的《Deep Learning》和 《C++ primer》，但这两本书都没看完。想要战胜这两个大部头还需要更多的时间。还有一门课是我2017年最大的收获:<a href="https://zh.gluon.ai/" target="_blank" rel="noopener">动手学深度学习</a>，我也可以算是会用mxnet了。之前有在看tensorflow，奈何没有项目的需求，看这个纯粹是兴趣支撑，而没有任务驱动的学习总是容易半途而废的。而从实战角度出来学mxnet就比较愉快了。</p><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><p>这两天一直有焦虑感，觉得还有很多的东西没学会，可能是我没事逛知乎逛的太多了，看见很多牛逼的人，就想着我也有那么牛逼就好了。我觉得焦虑在一定程度上是一件好事，说明你对现状有点担心，用一句被用烂了的话说就是你的实力配不上你的野心。之前也看过一个说法，大致意思是人与人之间的差距并不是一天，一个月或者一年就形成的，而是经过多年的积累，所以，一直的热情并不会对自己起多大作用，重要的是坚持，可最难的也是坚持。所以当前更重要的是放松心态，一步一步地走下去。2018年我希望怀着一颗平常心，脚踏实地慢慢地成长，能做到不骄不躁就非常好了。2018会是我工作的第一年，希望我能坚持锻炼，坚持学习，坚持读书，好好工作，努力提升自己，为以后回南京攒点资本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;我的2017-忙碌&quot;&gt;&lt;a href=&quot;#我的2017-忙碌&quot; class=&quot;headerlink&quot; title=&quot;我的2017-忙碌&quot;&gt;&lt;/a&gt;我的2017-忙碌&lt;/h2&gt;&lt;p&gt;2017是非常忙碌的一年，刚过完年就投入了找实习的大军中，好不容易找到了一个实习，过
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>git note</title>
    <link href="http://matafight.github.io/2018/01/12/git-note/"/>
    <id>http://matafight.github.io/2018/01/12/git-note/</id>
    <published>2018-01-12T06:00:24.000Z</published>
    <updated>2018-01-12T06:13:36.229Z</updated>
    
    <content type="html"><![CDATA[<h2 id="git环境中的三个重要概念"><a href="#git环境中的三个重要概念" class="headerlink" title="git环境中的三个重要概念"></a>git环境中的三个重要概念</h2><ul><li>工作目录</li><li>本地仓库</li><li>远程仓库<br>用户在工作目录中更改的文件通过stage 和 commit 提交到本地仓库，本地仓库通过push命令同步到远程仓库</li></ul><h2 id="checkout命令"><a href="#checkout命令" class="headerlink" title="checkout命令"></a>checkout命令</h2><ul><li>新建分支: git checkout -b ‘branch-name’</li><li>切换分支： git checkout ‘branckname’</li><li>查看分支： git branch</li><li>如果commit了错误的代码怎么办？ 首先通过git log 命令查看历史版本，之后通过 git checkout \<hash-name\>切换到指定位置。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/git1.png" alt=""><br><img src="http://7xiegr.com1.z0.glb.clouddn.com/git2.png" alt=""><br>HEAD指针指向新的位置后，如果在此处新建一个提交会产生一个新的未命名的分支<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/git3.png" alt=""><br>如果此时通过 git checkout master命令回到主分支的话，该新的提交会丢失，怎么做才能不丢失呢？通过git checkout -b newbranchname 在该新的提交的基础上新建一个分支。这样即使回到了主分支，newbranchname这个分支也不会丢失。在master分支通过执行 git merge newbranchename 合并分支。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/git4.png" alt=""></hash-name\></li></ul><h2 id="删除本地和远程库上的master-分支？（主要是因为已经在dev分支上做了很多改变，merge-到-master-分支上会产生很多麻烦，不如直接删除master-分支）"><a href="#删除本地和远程库上的master-分支？（主要是因为已经在dev分支上做了很多改变，merge-到-master-分支上会产生很多麻烦，不如直接删除master-分支）" class="headerlink" title="删除本地和远程库上的master 分支？（主要是因为已经在dev分支上做了很多改变，merge 到 master 分支上会产生很多麻烦，不如直接删除master 分支）"></a>删除本地和远程库上的master 分支？（主要是因为已经在dev分支上做了很多改变，merge 到 master 分支上会产生很多麻烦，不如直接删除master 分支）</h2><ul><li><p>git checkout -b dev //新建并切换到 dev分支</p></li><li><p>先git checkout dev 到 dev分支</p></li><li><p>在github网页端设置 default branch 为 dev 分支</p></li><li>git branch -D  master //删除本地master分支</li><li><p>git push origin :master //删除远程master 分支</p></li><li><p>然后可以重新新建一个master 分支，push当前代码到master 分支上，再设置default branch为master 分支</p></li></ul><h2 id="如何只clone-除了master分支之外的其他分支？"><a href="#如何只clone-除了master分支之外的其他分支？" class="headerlink" title="如何只clone 除了master分支之外的其他分支？"></a>如何只clone 除了master分支之外的其他分支？</h2><ul><li><p>先使用标准的clone 命令，git clone XXX.git</p></li><li><p>之后 git branch -a 查看所有分支，会显示:</p><ul><li>origin/master</li><li>origin/dev</li></ul></li><li><p>然后直接： git checkout origin/dev 切换到dev分支</p></li></ul><h2 id="如果不小心-commit-了一个错误的代码，或者merge出现了错误怎么恢复之前的状态？"><a href="#如果不小心-commit-了一个错误的代码，或者merge出现了错误怎么恢复之前的状态？" class="headerlink" title="如果不小心 commit 了一个错误的代码，或者merge出现了错误怎么恢复之前的状态？"></a>如果不小心 commit 了一个错误的代码，或者merge出现了错误怎么恢复之前的状态？</h2><ul><li>先用git -reflog 查看仓库commit状态，会看到这样的状态：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/git5.png" alt=""></li><li>如果想恢复到某个具体地时刻点，可以强制转换:</li><li>git reset –hard @{2}，表示恢复到 HEAD@{2}时的状态</li></ul><h2 id="git-fetch-git-merge-与-git-pull-的区别与联系"><a href="#git-fetch-git-merge-与-git-pull-的区别与联系" class="headerlink" title="git fetch,git merge 与 git pull 的区别与联系"></a>git fetch,git merge 与 git pull 的区别与联系</h2><ul><li>git fetch &lt;远程主机名&gt;&lt;分支名&gt; 用来获得远程主机上的分支的内容<ul><li>git fetch origin master 取回origin 主机上的master 分支</li></ul></li><li>git branch -r ,可以用来查看远程分支</li><li><p>git branch -a 查看所有分支，包括本地分支和远程分支</p></li><li><p>当 使用命令 git fetch origin master 之后，可以 git checkout -b newBran origin/master 创建基于 origin/master分支的本地分支</p></li><li><p>注意，fetch下来的分支并没有与本地分支合并，</p></li><li><p>需要使用 git merge origin/master 将其与本地分支合并</p></li><li><p>而git pull origin master 命令，则是上述两个命令的合体版本，先获得origin/master 上的内容然后与本地分支合并</p></li></ul><h2 id="git-push-命令"><a href="#git-push-命令" class="headerlink" title="git push 命令"></a>git push 命令</h2><ul><li><p>git push &lt;远程主机名&gt; &lt;本地分支名&gt;：&lt;远程分支名&gt;</p><ul><li><p>如果省略远程分支名，则表示将本地分支推送到与之有追踪关系的远程分支，如果该远程分支不存在，就会新建一个远程分支。</p></li><li><p>如果省略本地分支名，表示删除远程分支</p><ul><li>git push origin :master</li></ul></li></ul></li></ul><h2 id="如何将git-add-与-git-commit-缩成一步写？"><a href="#如何将git-add-与-git-commit-缩成一步写？" class="headerlink" title="如何将git add 与 git commit 缩成一步写？"></a>如何将git add 与 git commit 缩成一步写？</h2><ul><li>使用git commit  -am “message”<h2 id="git-查看远程其他分支，并切换到其他分支"><a href="#git-查看远程其他分支，并切换到其他分支" class="headerlink" title="git 查看远程其他分支，并切换到其他分支"></a>git 查看远程其他分支，并切换到其他分支</h2></li><li>git branch -r 查看</li><li>git check origin/otherbranch 切换</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;git环境中的三个重要概念&quot;&gt;&lt;a href=&quot;#git环境中的三个重要概念&quot; class=&quot;headerlink&quot; title=&quot;git环境中的三个重要概念&quot;&gt;&lt;/a&gt;git环境中的三个重要概念&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;工作目录&lt;/li&gt;
&lt;li&gt;本地仓库&lt;/
      
    
    </summary>
    
      <category term="git" scheme="http://matafight.github.io/categories/git/"/>
    
    
      <category term="git" scheme="http://matafight.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Chapter16</title>
    <link href="http://matafight.github.io/2017/11/05/Deep-Learning-Chapter16/"/>
    <id>http://matafight.github.io/2017/11/05/Deep-Learning-Chapter16/</id>
    <published>2017-11-05T06:46:46.000Z</published>
    <updated>2017-12-12T13:56:25.505Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/mata123/p/7787395.html" target="_blank" rel="noopener">http://www.cnblogs.com/mata123/p/7787395.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/mata123/p/7787395.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cnblogs.com/mata123/p/7787395.html&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://matafight.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>CNN结构的发展</title>
    <link href="http://matafight.github.io/2017/10/19/CNN%E7%BB%93%E6%9E%84%E7%9A%84%E5%8F%91%E5%B1%95/"/>
    <id>http://matafight.github.io/2017/10/19/CNN结构的发展/</id>
    <published>2017-10-19T11:41:19.000Z</published>
    <updated>2017-12-12T13:56:25.503Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近李沐大神开了一门关于深度学习的直播课程，每周六都在斗鱼上直播，这门课不仅介绍深度学习的理论知识，更重要的是还会给大家演示实战操作，包括各种模型的代码，采用的深度学习框架为Mxnet，这门课不仅使大家对深度学习有了更深一步的了解，毕竟有了上手操作的机会，由于框架是Mxnet，其实在一定程度上也推广了Mxnet的使用，可谓一举两得。当然，对于我来说就是既学习了理论又掌握了一个新的框架，简直不要太爽。</p><p>这篇文章的主要内容是回顾比较有名的几个卷积神经网络结构以及它们的动机。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>AlexNet是2012年ImageNet 图像识别挑战的冠军模型，AlexNet是一个八层的卷积神经网络，其中前五层是卷积层，后两层是全连接层，最后一层是输出层。在当时，这个模型的参数可以说是非常巨大了，所以当时的AexnNet采用了双数据流的设计，采用了两个GPU，每个GPU存储每层网络的一半节点，同时在一些特定层相互通信。这个设计是由于当时硬件条件的限制而产生的，现在一般都用不着这样设计了。<br>从论文中截取的网络结构如下：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/Alexnet.PNG" alt=""></p><p>假设输入为一张图片，那么该图片卷积之后的大小的计算公式为：</p><p>$$<br>\frac{N + 2\times padding - F}{stride} +1<br>$$<br>其中N是图片的长或宽，F是卷积核的长或宽。</p><p>输入层： 输入图片的大小是224x224x3。</p><p>第一层卷积：第一层一共有96个卷积核，kernel_size : 11x11，stride :4x4 . 之后接一个pooling操作，kernel_size: 3x3,stride:2x2,padding: 0x0</p><p>第二层卷积： 第二层有256个卷积核, kernel_size: 5x5, stride:1x1, padding:2x2, 之后也是接一个pooling 操作： kernel_size : 3x3, stride:2x2, padding:0x0</p><p>第三层卷积： 第三层有384个卷积核，kernel_size: 3x3, stride:1x1, padding:1x1, 之后不接pooling层</p><p>第四层卷积： 第四层卷积与第三层参数一致</p><p>第五层卷积： 第五层有256个卷积核，kernel_size:3x3, stride:1x1,padding:1x1, 之后不接pooling层</p><p>第六层： 全连接层,隐节点数为 4096，之后接一个dropout层</p><p>第七层： 全连接层，隐节点数为4096, 之后接一个dropout层</p><p>第八层：输出层, 结点个数为10(因为分类任务有10类)。</p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>AlexNet在构建过程中需要手写代码一层一层地构建，而且每一层都比较复杂，参数比较多。VGG的思路将神经网络分成多个block，每个block里面都是重复结构的网络层，这样VGG在构建过程中可以利用编程语言的循环操作，在每个block内都循环地增加相同的网络。最后将所有block组合成最终的网络结构,而且最终可以生成比较深的网络。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/vggnet.PNG" alt=""></p><h2 id="NiN-Network-in-Network"><a href="#NiN-Network-in-Network" class="headerlink" title="NiN(Network in Network)"></a>NiN(Network in Network)</h2><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/NiN_compare.PNG" alt=""><br>在一般的CNN中，输入的图片在与卷积核卷积之后生成 feature map, 卷积核与图片的作用是线性的，NiN的想法是用一个多层感知机替代这个线性操作，从而非线性化卷积过程，能提取更多有用的特征。<br>NiN另一个特点是不再使用全连接层作为最后一层，对于一般的分类任务来说，输出层都是一个节点数为类别数的全连接层，全连接层会导致网络过于复杂，影响泛化能力。NiN使用了全局均值池化的操作作为最后一层的输出，假设类别数为10，NiN最后一层为一个10层的 feature map,对每一层做一个均值pooling作为10个类的分类指标。</p><p>具体在实现过程中可以用卷积核大小为1x1的卷积操作替代多层感知机。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/mlpconv.PNG" alt=""></p><p><a href="http://blog.csdn.net/hjimce/article/details/50458190" target="_blank" rel="noopener">http://blog.csdn.net/hjimce/article/details/50458190</a></p><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>2014年的ImageNet竞赛的冠军网络，GoogleNet借鉴了NiN的的MLP思想，也是用kernel size 为1x1的卷积操作替代MLP。<br>其中一个核心部件称为:Inception<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/inception.PNG" alt=""><br>GoogLeNet的特点是较复杂且比较深，每个Inception由四个并行的卷积块组成。不同大小的卷积核对应着获取输入数据不同粒度的特征，图中1x1大小的卷积操作的作用是一方面调整输入数据channel的个数，防止后面卷积操作参数过多，过拟合。另一方面类似于NiN中的MLP，起到了非线性化卷积操作的作用。</p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>2015年的ImageNet竞赛的最大赢家是ResNet(残差网络)，其第一次极大地提高了网络的深度，达到了上百层。提高网络深度的一大难题就是梯度消失，梯度在反向传播过程中无法传递到靠前的网络层，从而导致网络难训练。</p><p>ResNet的思想是采用跨层连接来解决梯度消失问题，其结构如下所示：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/res_%E8%B7%A8%E5%B1%82%E8%BF%9E%E6%8E%A5.PNG" alt=""></p><p>为什么叫残差网络呢？<br>将上面的结构拆成两个网络的和，其中最下面一层是共享的,如下图所示：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/resnet_%E5%B1%95%E5%BC%80.PNG" alt=""><br>在训练过程中，左边的网络更容易训练，所以这个网络会更快地收敛，这个网络没有拟合到的部分，可以称为残差，就被右边的网络抓取住，所以称为残差网络。直观上来看，即使加深网络，跨层连接仍然可以使得底层网络得到充分训练，从而不会让训练更难。</p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/dense_net.PNG" alt=""><br>DenseNet与 ResNet最大的区别是ResNet采用累加的方式去利用过去的信息，而DenseNet采用拼接的方式。</p><p><strong>DenseNet号称参数比ResNet参数更少？为什么？</strong></p><p>因为DenseNet每一层的output(也就是 feature map)都会concat到后续所有层的input上，每一层的input都是由前面所有层的output concat得到的。这个设计看似会产生很多参数，其实并没有，因为feature map不需要重新训练，而且，由于采用了这种直接拼接的方式(Resnet采用的是加和的方式)，信息保留的比较多，不需要通过增大feature map的层数来保留信息，所以feature map 的层数可以设置得比较小，所以参数较少。</p><p><strong>神经网络为什么致力于提高深度而不是提高宽度呢？毕竟网络较深时不好优化，而宽度较大却没有这个问题?</strong><br>因为增大宽度使得模型的复杂性急速上升，会overfitting。</p><p>这里只是列出来一些直觉上的和结构上的东西，并没有深入阐述，想要深入了解还需要去看原始论文，参考文献就是按照本文介绍网络的先后顺序排列的。</p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a><strong>参考</strong>：</h2><ol><li><a href="https://zh.gluon.ai/chapter_convolutional-neural-networks/index.html" target="_blank" rel="noopener">https://zh.gluon.ai/chapter_convolutional-neural-networks/index.html</a></li><li>Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</li><li>Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</li><li>Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</li><li>He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.<br>6</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近李沐大神开了一门关于深度学习的直播课程，每周六都在斗鱼上直播，这门课不仅介绍深度学习的理论知识，更重要的是还会给大家演示实战操作，包括各
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://matafight.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep learning chapter10</title>
    <link href="http://matafight.github.io/2017/09/25/Deep-learning-chapter10-part-1/"/>
    <id>http://matafight.github.io/2017/09/25/Deep-learning-chapter10-part-1/</id>
    <published>2017-09-25T12:20:49.000Z</published>
    <updated>2017-12-12T13:56:25.507Z</updated>
    
    <content type="html"><![CDATA[<p>最近已知在看Bengio的《Deep Learning》，感觉光看书效果不好，有时候你以为理解了，其实没有理解，所以我计划在看的过程中将中文翻译写下来，一方面加深印象和理解，另一方面也算锻炼了语言组织能力吧，由于这个博客显示公式总是有些问题，找了好久没找到原因，先暂时将文章发在博客园上，链接如下：<br>part1: <a href="http://www.cnblogs.com/mata123/p/7593933.html" target="_blank" rel="noopener">http://www.cnblogs.com/mata123/p/7593933.html</a><br>part2: <a href="http://www.cnblogs.com/mata123/p/7615971.html" target="_blank" rel="noopener">http://www.cnblogs.com/mata123/p/7615971.html</a><br>part3: <a href="http://www.cnblogs.com/mata123/p/7642589.html" target="_blank" rel="noopener">http://www.cnblogs.com/mata123/p/7642589.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近已知在看Bengio的《Deep Learning》，感觉光看书效果不好，有时候你以为理解了，其实没有理解，所以我计划在看的过程中将中文翻译写下来，一方面加深印象和理解，另一方面也算锻炼了语言组织能力吧，由于这个博客显示公式总是有些问题，找了好久没找到原因，先暂时将文章
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://matafight.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>广义线性模型</title>
    <link href="http://matafight.github.io/2017/09/17/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://matafight.github.io/2017/09/17/广义线性模型/</id>
    <published>2017-09-17T11:58:44.000Z</published>
    <updated>2017-12-12T13:56:25.575Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是广义线性模型"><a href="#什么是广义线性模型" class="headerlink" title="什么是广义线性模型"></a>什么是广义线性模型</h2><p>假设现在有一个预测任务，其输入是$x_i(1\le i \le n)$,对应的label 是$y_i$，我们知道可以用线性模型建模为:</p><p>$$<br>y = \omega^Tx+b<br>$$</p><p>广义线性模型一个最简单的解释是，存在一个单调可微的连接函数$g(\cdot)$满足:</p><p>$$<br>g(y) = \omega^Tx+b<br>$$</p><p>那么模型:$y = g^{-1}(\omega^Tx+b)$就是广义线性模型。</p><h2 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h2><p>上面说到了广义线性模型需要确定一个连接函数，那么连接函数怎么确定呢?当然单调可微的函数都符合条件，但是狭义上的广义线性模型是与指数分布族这个概念联系在一起的。假设样本标号$y$的分布属于指数分布族，那么就存在一个相对应的连接函数使得$(x,y)$可以用广义线性模型建模。</p><p>指数分布族的形式如下：</p><p>$$<br>p(y,\eta)=b(y)exp(\eta^TT(y)-a(\eta))<br>$$<br>其中$\eta = \omega^Tx$(将b也嵌入到$\omega$中)。</p><h2 id="从线性回归到逻辑斯谛回归"><a href="#从线性回归到逻辑斯谛回归" class="headerlink" title="从线性回归到逻辑斯谛回归"></a>从线性回归到逻辑斯谛回归</h2><p>很多分布都属于指数分布族，如高斯分布，二项分布，泊松分布等。这里以高斯分布和二项分布为例推导广义线性模型。</p><h3 id="高斯分布与线性模型"><a href="#高斯分布与线性模型" class="headerlink" title="高斯分布与线性模型"></a>高斯分布与线性模型</h3><p>假设$y=\omega^Tx+\epsilon$，其中$\epsilon \sim N(0,\sigma^2)$,则有y服从正态分布$y\sim N(\omega^Tx,\sigma^2)$,将这个正态分布展开:</p><p>$$<br>p(y;\eta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\eta)^2}{2\sigma^2})<br>$$<br>将其展开成指数分布族的形式:</p><p>$$<br>p(y;\eta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{y^2}{2\sigma^2})exp(\frac{y\omega^Tx}{\sigma^2}-\frac{(\omega^Tx)^2}{2\sigma^2})<br>$$</p><p>所以对应着指数分布族有:</p><p>$$b(y) =  \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{y^2}{2\sigma^2})<br>$$</p><p>$$T(y) = \frac{y}{\sigma^2}$$</p><p>$$<br>\eta= \omega^Tx<br>$$</p><p>$$<br>a(\eta) = \frac{(\omega^Tx)^2}{2\sigma^2}<br>$$<br>由第三行公式$\eta= \omega^Tx$可知线性模型的连接函数就是恒等函数</p><h3 id="二项分布与逻辑斯谛回归"><a href="#二项分布与逻辑斯谛回归" class="headerlink" title="二项分布与逻辑斯谛回归"></a>二项分布与逻辑斯谛回归</h3><p>假设$y\in{-1,1}$,也就是说这是一个二分类任务，则样本label服从二项分布:</p><p>$$<br>p(y)=\pi^y(1-\pi)^{(1-y)}$$<br>其中$\pi$表示样本为正例的概率，将其变换成指数分布族的形式:</p><p>$$<br>p(y) = exp(ylog(\pi)+(1-y)log(1-\pi))<br>$$</p><p>$$<br>=exp(ylog(\frac{y}{1-y})+log(1-\pi))<br>$$<br>则有:</p><p>$$<br>b(y) = 1<br>$$</p><p>$$<br>T(y) = y$$</p><p>$$<br>\eta=log(\frac{\pi}{1-\pi})<br>$$</p><p>$$<br>a(\eta) = log(1-\pi)<br>$$</p><p>由$\eta=log(\frac{\pi}{1-\pi})$可推出sigmoid函数:</p><p>$$<br>\pi = \frac{e^{\eta}}{1+e^{\eta}}<br>$$</p><p>因此对于二项分布来说，其连接函数是sigmoid函数。由此可推出Logistic Regression，也能看出LR的输出范围是0-1，可以看作是样本为正例的概率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是广义线性模型&quot;&gt;&lt;a href=&quot;#什么是广义线性模型&quot; class=&quot;headerlink&quot; title=&quot;什么是广义线性模型&quot;&gt;&lt;/a&gt;什么是广义线性模型&lt;/h2&gt;&lt;p&gt;假设现在有一个预测任务，其输入是$x_i(1\le i \le n)$,对应的lab
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machien learning" scheme="http://matafight.github.io/tags/machien-learning/"/>
    
  </entry>
  
  <entry>
    <title>gbdt,xgboost与lightgbm</title>
    <link href="http://matafight.github.io/2017/08/29/gbdt-xgboost%E4%B8%8Elightgbm/"/>
    <id>http://matafight.github.io/2017/08/29/gbdt-xgboost与lightgbm/</id>
    <published>2017-08-29T06:48:00.000Z</published>
    <updated>2017-12-12T13:56:25.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>梯度提升树实在提升树的基础上发展而来的一种使用范围更广的方法，当处理回归问题时，提升树可以看作是梯度提升树的特例(分类问题时是不是特例？)。 因为提升树在构建树每一步的过程中都是去拟合上一步获得模型在训练集上的残差。后面我们将会介绍，这个残存正好是损失函数的梯度，对应于GBDT每一步要拟合的对象。</p><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>在目标函数所在的函数空间中做梯度下降，即把待求的函数模型当作参数，每一步要拟合目标函数关于上一步获得的模型的梯度，从而使得参数朝着最小化目标函数的方向更新。</p><h3 id="一些特性"><a href="#一些特性" class="headerlink" title="一些特性"></a>一些特性</h3><ol><li>每次迭代获得的决策树模型都要乘以一个缩减系数，从而降低每棵树的作用，提升可学习空间。</li><li>每次迭代拟合的是一阶梯度。</li></ol><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost 是GBDT的一个变种，最大的区别是xgboost通过对目标函数做二阶泰勒展开，从而求出下一步要拟合的树的叶子节点权重（需要先确定树的结构），从而根据损失函数求出每一次分裂节点的损失减小的大小，从而根据分裂损失选择合适的属性进行分裂。</p><p>这个利用二阶展开的到的损失函数公式与分裂节点的过程是息息相关的。先遍历所有节点的所有属性进行分裂，假设选择了这个a属性的一个取值作为分裂节点，根据泰勒展开求得的公式可计算该树结构各个叶子节点的权重，从而计算损失减小的程度，从而综合各个属性选择使得损失减小最大的那个特征作为当前节点的分裂属性。依次类推，直到满足终止条件。</p><h3 id="一些特性-1"><a href="#一些特性-1" class="headerlink" title="一些特性"></a>一些特性</h3><ol><li>除了类似于GBDT的缩减系数外，xgboost对每棵树的叶子节点个数和权重都做了惩罚，避免过拟合</li><li>类似于随机森林，XGBoost在构建树的过程中，对每棵树随机选择一些属性作为分裂属性。</li><li><p>分裂算法有两种，一种是精确的分裂，一种是近似分裂算法，精确分裂算法就是把每个属性的每个取值都当作一次阈值进行遍历，采用的决策树是CART。近似分裂算法是对每个属性的所有取值进行分桶，按照各个桶之间的值作为划分阈值，xgboost提出了一个特殊的分桶策略，一般的分桶策略是每个样本的权重都是相同 的，但是xgboost使每个样本的权重为损失函数在该样本点的二阶导(泰勒展开不应该是损失函数关于模型的展开吗？为什么会有在该样本点的二阶导这种说法？ 因为模型是对所有样本点都通用的，把该样本输入到二阶导公式中就可以得到了)。</p></li><li><p>xgboost添加了对稀疏数据的支持，在计算分裂收益的时候只利用没有missing值的那些样本，但是在推理的时候，也就是在确定了树的结构，需要将样本映射到叶子节点的时候，需要对含有缺失值的样本进行划分，xgboost分别假设该样本属于左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为该样本的分裂方向。</p></li><li><p>xgboost在实现上支持并行化，这里的并行化并不是类似于rf那样树与树之间的并行化，xgboost同boosting方法一样，在树的粒度上是串行的，但是在构建树的过程中，也就是在分裂节点的时候支持并行化，比如同时计算多个属性的多个取值作为分裂特征及其值，然后选择收益最大的特征及其取值对节点分裂。</p></li><li><p>xgboost 在实现时，需要将所有数据导入内存，做一次pre-sort（exact algorithm），这样在选择分裂节点时比较迅速。</p></li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。</li><li>预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)</li></ol><h2 id="lightGBM"><a href="#lightGBM" class="headerlink" title="lightGBM"></a>lightGBM</h2><p><a href="https://github.com/Microsoft/LightGBM/wiki/Features" target="_blank" rel="noopener">https://github.com/Microsoft/LightGBM/wiki/Features</a><br>关于lightGBM的论文目前并没有放出来，只是从网上一些信息得出以下的一些与xgboost不同的地方：</p><ol><li><p>xgboost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是xgboost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。</p></li><li><p>lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。<br> -. 内存上优势：很明显，直方图算法的内存消耗为(#data<em> #features </em> 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 <em> #data </em> #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。<br> -. 计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)</p></li><li><p>直方图做差加速<br>  -.  一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。</p></li><li><p>lightgbm支持直接输入categorical 的feature<br>  -. 在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。</p></li><li><p>但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？<br> -. xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。<br> -. lightgbm做了cache优化？</p></li><li><p>lightgbm哪些方面做了并行？<br>-. feature parallel</p><pre><code>一般的feature parallel就是对数据做垂直分割（partiion data vertically，就是对属性分割），然后将分割后的数据分散到各个workder上，各个workers计算其拥有的数据的best splits point, 之后再汇总得到全局最优分割点。但是lightgbm说这种方法通讯开销比较大，lightgbm的做法是每个worker都拥有所有数据，再分割？（没懂，既然每个worker都有所有数据了，再汇总有什么意义？这个并行体现在哪里？？）</code></pre><p>-. data parallel </p><pre><code>传统的data parallel是将对数据集进行划分，也叫 平行分割(partion data horizontally)， 分散到各个workers上之后，workers对得到的数据做直方图，汇总各个workers的直方图得到全局的直方图。 lightgbm也claim这个操作的通讯开销较大，lightgbm的做法是使用”Reduce Scatter“机制，不汇总所有直方图，只汇总不同worker的不同feature的直方图(原理？)，在这个汇总的直方图上做split，最后同步。</code></pre></li></ol><p>参考：<a href="https://github.com/Microsoft/LightGBM/wiki/Features" target="_blank" rel="noopener">https://github.com/Microsoft/LightGBM/wiki/Features</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GBDT&quot;&gt;&lt;a href=&quot;#GBDT&quot; class=&quot;headerlink&quot; title=&quot;GBDT&quot;&gt;&lt;/a&gt;GBDT&lt;/h2&gt;&lt;p&gt;梯度提升树实在提升树的基础上发展而来的一种使用范围更广的方法，当处理回归问题时，提升树可以看作是梯度提升树的特例(分类问题时
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>kaggle竞赛总结</title>
    <link href="http://matafight.github.io/2017/08/05/kaggle%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    <id>http://matafight.github.io/2017/08/05/kaggle竞赛总结/</id>
    <published>2017-08-05T12:29:11.000Z</published>
    <updated>2017-12-12T13:56:25.541Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>最近刚参加了一个kaggle比赛(Mercedes-Benz Greener Manufacturing),据赛题介绍主要的任务是预测一辆刚出厂的汽车做完检测所需要的时间，从而能够合理安排检测任务，节省能源？创建一个绿色环保的世界？？<br>我的最终排名是 273/3835,进了top10%，得到了一个Bronze medal,是我得到的第一个奖牌，开心。<br>在这里总结一下参加这个比赛的收获。</p><h2 id="特征部分"><a href="#特征部分" class="headerlink" title="特征部分"></a>特征部分</h2><p>这个比赛的特征都是经过脱敏的，所以你不知道各个特征的具体意思，这一点对于特征工程来说就比较难受了，这个数据与在Quora比赛中使用的数据类型不同，Quora数据的一个样本是两个句子，任务是判断这两个问题相同或不相同，所以任务的很大一部分都是做特征工程，提取特征，比如两个句子的cos距离，tf-idf特征，pearson相关系数，通过word2vec提取出来的向量等等都可以用作Quora任务的特征，但是，Benz竞赛的数据只是给定的脱敏数据，具体的特征工程需要自己尝试。</p><p>这次比赛的重点我放在了模型融合和集成学习方面，所以特征方面探索得比较少，这也是未来需要改进的方向。<br>我的做法是对于categorical features，直接采用one-hot 编码，对于numerical features,分别使用了PCA,SVD，ICA，随机投影等方法抽取特征，并将抽取出的特征与原有特征连接在一起组成新的特征，之后的模型融合也基于这些新的特征。</p><p>后来第二名在讨论区中公布了他的<a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36390" target="_blank" rel="noopener">解决方案</a>,他通过类似于wrapper的方式做特征选择(不同于filter方式，wrapper的feature subset的好坏由training algorihtm’s crossvalid performance决定，而filter方式的feature subset选择与training algorithm 相互独立，subset的选择是通过其他方式),我在这里贴一下他的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> choice</span><br><span class="line">n_elements = train.shape[<span class="number">0</span>]</span><br><span class="line">feats = train.columns.values[<span class="number">9</span>:]</span><br><span class="line">n_feats = len(feats)</span><br><span class="line">weights = np.array(np.zeros(n_feats))</span><br><span class="line">weights += <span class="number">1</span>/n_feats</span><br><span class="line"></span><br><span class="line">best_feats = []</span><br><span class="line">best_score = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">3000</span>    <span class="comment"># number of rounds / feature bags</span></span><br><span class="line">n_features = <span class="number">10</span> <span class="comment"># try out different values </span></span><br><span class="line">wt_g = <span class="number">.2</span>  <span class="comment">#weight growth rate</span></span><br><span class="line">w_threshold = <span class="number">.2</span>  <span class="comment">#weight to add feature</span></span><br><span class="line">kf = KFold(n_elements, n_folds=<span class="number">3</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line">y = train[<span class="string">'y'</span>].values</span><br><span class="line">scores = []</span><br><span class="line">top_feats = np.array([])</span><br><span class="line">new_feats = np.array([])</span><br><span class="line"><span class="keyword">for</span> i, ind <span class="keyword">in</span> enumerate(list(range(epochs))):</span><br><span class="line"></span><br><span class="line">    sample_feat_ids = choice(a=n_feats, size=n_features, </span><br><span class="line">                             replace=<span class="keyword">False</span>, p=weights)</span><br><span class="line">    sample_feats = np.append(top_feats, feats[sample_feat_ids])</span><br><span class="line">    tst_P = np.array(np.zeros(n_elements))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> trn, tst <span class="keyword">in</span> kf: </span><br><span class="line">        X = train.loc[:,sample_feats]</span><br><span class="line">        trn_X, tst_X = X.iloc[trn,:], X.iloc[tst,:]</span><br><span class="line">        trn_Y, tst_Y = clipped_y[trn], clipped_y[tst]</span><br><span class="line">        <span class="comment">#mod = SVR(C=50, epsilon=3, gamma=.2)</span></span><br><span class="line">        <span class="comment">#mod = GBR(alpha=.01, n_estimators=50, max_depth=5, min_samples_leaf=15, subsample=.5, random_state=1776)</span></span><br><span class="line">        mod = RFR(n_estimators=<span class="number">100</span>, max_depth=<span class="number">12</span>, max_features=<span class="number">20</span>, min_samples_leaf=<span class="number">4</span>, n_jobs=<span class="number">2</span>, random_state=<span class="number">1776</span>)</span><br><span class="line">        mod.fit(trn_X, trn_Y)</span><br><span class="line">        tst_P[tst] = mod.predict(tst_X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># I don't want to overfit to outliers so I am clipping all y's at 155</span></span><br><span class="line">    tst_rsq = r2(y_pred=tst_P, y_true=clipped_y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ind &gt; <span class="number">29</span>:</span><br><span class="line">        scores.append(tst_rsq)</span><br><span class="line">        ma_rsq = np.mean(scores[<span class="number">-30</span>:])</span><br><span class="line">        <span class="keyword">if</span> ind % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">            print(ind, ma_rsq, tst_rsq)</span><br><span class="line">        <span class="keyword">if</span> tst_rsq &gt; ma_rsq:</span><br><span class="line">            weights[sample_feat_ids] *= (<span class="number">1</span>+wt_g)</span><br><span class="line">            sum_w = np.sum(weights)</span><br><span class="line">            weights /= sum_w</span><br><span class="line">            <span class="keyword">if</span> tst_rsq &amp;gt; best_score:</span><br><span class="line">                best_score = tst_rsq</span><br><span class="line">                best_feats = sample_feats</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weights[sample_feat_ids] *= (<span class="number">1</span>-wt_g)</span><br><span class="line">            sum_w = np.sum(weights)</span><br><span class="line">            weights /= sum_w</span><br><span class="line">        mx_w = np.max(weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add feature to the top feats if weight is &amp;gt; threshold</span></span><br><span class="line">        <span class="keyword">if</span> mx_w &gt; w_threshold:</span><br><span class="line">            feat_imps = pd.Series(index = feats, data = weights)</span><br><span class="line">            new_feats = feat_imps[feat_imps &gt; w_threshold].index.values</span><br><span class="line">            top_weights = feat_imps[feat_imps &gt; w_threshold].values</span><br><span class="line">            top_feats = np.append(top_feats, new_feats)</span><br><span class="line">            feats = list(feats)</span><br><span class="line">            weights = list(weights)</span><br><span class="line">            <span class="keyword">for</span> f,w <span class="keyword">in</span> zip(new_feats,top_weights):</span><br><span class="line">                print(f, w)</span><br><span class="line">                feats.remove(f)</span><br><span class="line">                weights.remove(w)</span><br><span class="line">            n_feats = len(weights)</span><br><span class="line">            feats = np.array(feats)</span><br><span class="line">            weights = np.array(weights)</span><br><span class="line">            sum_w = np.sum(weights)</span><br><span class="line">            weights /= sum_w</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ind % <span class="number">25</span> == <span class="number">0</span>: </span><br><span class="line">            print(mx_w)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        scores.append(tst_rsq)</span><br><span class="line">        print(ind, tst_rsq)</span><br></pre></td></tr></table></figure></p><p>不过这种选择方法的时间复杂度还是挺高的。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>我采用了stacking的集成方式(stacking 与 blending的区别在于blending首先将数据划分为两部分，其中一部分用来训练，然后用训练好的模型预测另一部分，将这一部分的预测值当成下一层的输入，而stacking则是通过cross validation的方法预测所有的数据，然后将所有预测值输入到下一层)，blending的缺点是当数据量较少的时候，第二层的训练数据太少，优点是两层数据不相关，避免过拟合效果好，但是由于stacking是采用交叉验证的方式生成预测值，所以也能避免过拟合。</p><p>stacking采用了如下子模型：</p><ul><li>RandomForest</li><li>Gbregressor</li><li>Lasso</li><li>ridge</li><li>SVR</li></ul><p>stacking的方式可以参考我上一篇博客，代码可以参看我的<a href="https://github.com/Matafight/Kaggle" target="_blank" rel="noopener">github</a>。</p><p>我最终的结果还参考了public kernel上另一个比较好的<a href="https://www.kaggle.com/hakeem/stacked-then-averaged-models-0-5697?scriptVersionId=1236940/code" target="_blank" rel="noopener">Solution</a>,基本上是将我的预测值与这个kernel的预测值做了一个平均，这也可以看作是一种集成方式吧。</p><p>其实我觉得stacking也算是一种黑盒子吧，把那么多方法“堆”在一起，虽然有效果，但也是说不清道不明的原因，有一种说法我觉得还是比较有道理的，第二层的模型相当于对第一层的输出做组合，组合出最好的结果,所以第二层一般采用线性组合。这就是集成的思想。</p><p>集成学习最重要的就是保证各个分类器有足够的diversity和accuracy，要有足够的差异性和准确性，不能都是猪队友，也不能都是同一个行业的精英。需要注意的是同一个分类器的不同参数配置也可以看作是不同的模型。</p><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li><p>kernel和discussion经常会有人提供新的想法和证据来分析这个问题和数据，这是个学习数据处理的好地方。</p></li><li><p>不要太看重public LB，因为最终的结果由private LB决定，一般public LB占总测试集的20%左右，private LB的结果占80%左右，很多情况下会过拟合到public LB上去，此时最好相信本地的local CV score，特别是一些比赛可以通过public LB的反馈来获取public LB所采用测试集的分布和其他信息，比如对于二分类问题，提交全零的结果上去，如果PLB采用的评价准则为01误差，就可以得到PLB测试集的正负样本分布了，知道了样本分布就可以提高PLB上的分数了。但是这个分数对于Private LB没有意义，只会过拟合。</p></li><li><p>怎么根据样本分布提高分数呢？<br>我们知道对于类别不平衡问题有好几种解决方法。1. 重采样，降采样。2. rescaling，再缩放，比如对于logistic regression，y为样本为正例的概率， y/(1-y)&gt;1时为正例，y/(1-y)&lt;1时为负例，这个结果建立在正负样本平衡的基础上，如果正负不平衡，假设正例个数为m+，负例个数为m-,那么根据观测值得到的阈值为m+/m-， 这里假设观测值为总体值的无偏估计，所谓再缩放就是将1替换为m+/m-，重新判断正负。3.采用auc等对不平衡数据不敏感的度量方式。</p></li><li><p>特征还是比模型更重要的，模型基本上用来用去都是集成,stacking，xgboost等，如何做特征选择，如何分析特征还是最重要的。我这次这方面下的功夫比较少。</p></li><li><p>自动化，自动化，自动化脚本非常重要。</p></li><li><p>在对模型结果做weighted-averaging的时候权重该怎么选择？</p><ol><li>可以根据两个模型的cv score的比值分配权重</li><li>暴力搜索，然后交叉验证，虽然暴力但也是一种方法</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;最近刚参加了一个kaggle比赛(Mercedes-Benz Greener Manufacturing),据赛题介绍主要的任务是预测一辆刚
      
    
    </summary>
    
      <category term="kaggle" scheme="http://matafight.github.io/categories/kaggle/"/>
    
    
      <category term="machine learning" scheme="http://matafight.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>python中的编码问题</title>
    <link href="http://matafight.github.io/2017/06/30/python%E4%B8%AD%E7%9A%84%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/"/>
    <id>http://matafight.github.io/2017/06/30/python中的编码问题/</id>
    <published>2017-06-30T03:08:55.000Z</published>
    <updated>2018-01-07T07:55:49.585Z</updated>
    
    <content type="html"><![CDATA[<p>在写python程序的时候常常会因为字符编码问题而出错，尤其是在读取文件的时候，一般需要按照文件本身的编码来读取，否则非常容易出错，一般我们在python代码的第一行都声明编码方式为utf-8，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding:utf-8_*_</span></span><br></pre></td></tr></table></figure></p><p>如果没有申明编码方式的话，程序默认编码为ascii，而ascii不支持中文显示。当然这只是众多编码问题中的一种，实际运行时还会发生很多意想不到的问题，因此，搞懂python的编码规则很有必要。</p><p>python2和python3的编码总的来说是不同的。下面分开介绍python2和python3中的编码。</p><h2 id="python2中的编码"><a href="#python2中的编码" class="headerlink" title="python2中的编码"></a>python2中的编码</h2><p>刚开始发明计算机的时候，计算机只有ascii这一种编码方式，而ascii是一种单字节编码系统，也就是说每个字符只需要用一个字节来存储，计算机也很好识别，因为一个字节就对应着一个字符。但是随着计算机在全世界的发展，世界各地的人们对于其他字符有迫切的需求，比如中文输入，那么就需要对中文编码，因此发展出来了GBK，这种编码需要多个字节存储一个字符，不同的编码也对计算机处理字符串带来了困难。因此，后来人们发明了unicode编码来一统江湖，unicode本质上来说与ascii，gbk这种编码方式不同，因为unicode本质上存储的是字符串，而不是ascii这种字节串，所有的编码方式都可以通过decode方法转换成unicode字符串，而unicode也可以通过encode方法编码成别的字节串。</p><p>在python2中，关于字符串的主要有两个类，一个是str，一个是unicode，str可以有不同的编码，如ascii，utf-8等，我理解的unicode就是字符串，而str是字节串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=<span class="string">u"中文"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(a)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=a.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(b)</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure></p><p>可以看出”中文”的unicode长度就是2，而utf-8编码的长度是6，表示占用了6个字节。<br>将字符串存储到文件中时一般要存储经过编码的字符串，而不是直接存储unicode，如果没有显式编码，程序会采用默认的编码方式存储字符串。而读取文件内容时，也要显式指定文件的编码方式读取，读进来的是对应编码的字符串，一般为了操作方便，需要将输入内容decode成unicode。</p><h2 id="python3中的编码"><a href="#python3中的编码" class="headerlink" title="python3中的编码"></a>python3中的编码</h2><p>python3默认编码方式为unicode，python3中字符串也是有两个类，一个是bytes,另一个是str，不过python3中的str与python2中的str不一样，python3中的str默认就是unicode编码的，也就是说在python3中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a=<span class="string">"中文"</span></span><br></pre></td></tr></table></figure></p><p>a就是默认的unicode编码，这样极大地方便了我们处理字符串，如果在python2中的话，a就是utf-8编码了(需要在第一行声明编码方式)，而在python2程序中处理的字符串一般都需要先decode成unicode，在python3中默认读入的就是unicode，因此会减少很多麻烦。bytes就是对应与python2中的str，是指采用别的编码的字符串如utf-8等，声明bytes类型的字符串需要显式声明。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a=<span class="string">b"中文"</span></span><br></pre></td></tr></table></figure></p><p>不过这行代码是错的，bytes类型只能接受ascii字符串，不能接受非ascii字符串，如果非要用这种声明方式的话，需要将中文转换成对应编码的十六进制，bytes中存储的就是各个字节。</p><h2 id="十六进制与二进制字符串的转换"><a href="#十六进制与二进制字符串的转换" class="headerlink" title="十六进制与二进制字符串的转换"></a>十六进制与二进制字符串的转换</h2><p>这个问题主要来自于我最近看了一个用tensorflow识别知乎验证码的一个project。其中一个脚本的功能就是根据给定的字库，随机生成一些正常的或者上下颠倒的汉字。由于原始代码是只能运行在python2.x上的，我需要将其改为python3.x的版本。其中涉及到字符串编码的操作比较令人费解。虽然我成功地在python3.x版本上运行了该代码。但我对其背后的编码转换还是非常困惑。</p><p>python2版本的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str2.decode(<span class="string">'hex'</span>).decode(<span class="string">'gb2312'</span>)</span><br></pre></td></tr></table></figure></p><p>python3版本的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">str2b  = codecs.decode(str2,<span class="string">'hex'</span>)</span><br><span class="line">str2c = codecs.decode(str2b,<span class="string">'gb2312'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/python3_hex.PNG" alt=""><br>在python3中，str2默认是unicode编码的，str2可以decode成str2b，str2b具体的编码取决于当前环境的默认编码，比如我在cmd中运行python3程序，str2b的编码是ascii的。之后利用gb2312编码decode成unicode编码。</p><p><strong>这里有一个疑问是</strong>：当decode参数是hex时，为什么会将unicode解码成了ascii编码的字符。这个结果说明hex并不是一种编码格式，否则是不能这样转换的，那么hex作用是什么呢？</p><p>为了解决上述疑惑，需要理解<a href="https://docs.python.org/3.6/library/binascii.html#binascii.b2a_hex" target="_blank" rel="noopener">binascii</a>这个模块的作用。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/ascii.PNG" alt=""><br>简单来说，binascii模块的作用是将字符串在ascii和二进制表示之间进行切换。这个模块中包括两个函数:</p><ul><li>binascii.b2a_hex(data)</li><li>binascii.a2b_hex(data)</li></ul><p>b2a_hex 的作用是将一个字符转换成两个字符，怎么操作呢？假设要转换的字符串是”cleb”，这个字符串在内存中每个字符占一个字节，将其转换为十六进制，那么每个字符对应着十六进制占两个字符。b2a_hex会将”cleb”转化为”636c6562”,注意在python3中该函数仅接受byte类型的字符串，而不接受str(unicode)类型。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/b2a_hex.PNG" alt=""></p><p>a2b_hex的作用正好与b2a_hex的作用相反。将输入字符串中每两个字符转换成一个字符。其只接受unicode编码的ascii字符。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/a2b_hex.PNG" alt=""><br>而codecs.decode(str,’hex’)和codecs.encode(str,’hex’)分别是调用的底层函数a2b_hex和b2a_hex。所以在python3中，codecs.encode(str,’hex’)与b2a_hex一样仅接受byte类型字节串，codecs.decode(str,’hex’)仅接受unicode字符串。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/python3_codecs_encode_hex.PNG" alt=""><br><img src="http://7xiegr.com1.z0.glb.clouddn.com/python3_codecs_decode_hex.PNG" alt=""></p><p>参考：<br><a href="https://www.crifan.com/summary_python_string_encoding_decoding_difference_and_comparation_python_2_x_str_unicode_vs_python_3_x_bytes_str/" target="_blank" rel="noopener">https://www.crifan.com/summary_python_string_encoding_decoding_difference_and_comparation_python_2_x_str_unicode_vs_python_3_x_bytes_str/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在写python程序的时候常常会因为字符编码问题而出错，尤其是在读取文件的时候，一般需要按照文件本身的编码来读取，否则非常容易出错，一般我们在python代码的第一行都声明编码方式为utf-8，&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;ta
      
    
    </summary>
    
      <category term="python" scheme="http://matafight.github.io/categories/python/"/>
    
    
      <category term="编码" scheme="http://matafight.github.io/tags/%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>kaggle经验</title>
    <link href="http://matafight.github.io/2017/06/24/kaggle%E7%BB%8F%E9%AA%8C/"/>
    <id>http://matafight.github.io/2017/06/24/kaggle经验/</id>
    <published>2017-06-24T06:24:29.000Z</published>
    <updated>2017-12-12T13:56:25.543Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了kaggle的一个比赛，看了好多关于数据处理，特征分析以及集成学习的一些内容，这些都是数据挖掘大神总结出来的经验，在这里总结一下我的收获，以及谈一下我自己的一些想法。</p><p>参加数据挖掘比赛一般要从以下几个方面着手：</p><h3 id="数据探索与预处理"><a href="#数据探索与预处理" class="headerlink" title="数据探索与预处理"></a>数据探索与预处理</h3><p>所谓数据探索就是利用各种图表直观地观察各种分布情况，从而为接下来的数据预处理提供一个预处理的方向。</p><ol><li><p>首先分析数据，统计各个类型特征的个数，以及统计缺失值情况</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_train = pd.read_csv(<span class="string">'../input/train.csv'</span>)</span><br><span class="line"><span class="comment"># distinguish numerical attributes and object attributes and target column</span></span><br><span class="line">dtype_df = df_train.dtypes.reset_index()</span><br><span class="line">dtype_df.columns = [<span class="string">"Count"</span>, <span class="string">"Column Type"</span>]</span><br><span class="line">summary_dtypes = dtype_df.groupby(<span class="string">"Column Type"</span>).aggregate(<span class="string">'count'</span>).reset_index()</span><br><span class="line">print(summary_dtypes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#样例输出</span></span><br><span class="line"><span class="comment">#  Column Type  Count</span></span><br><span class="line"><span class="comment">#0       int64    369</span></span><br><span class="line"><span class="comment">#1     float64      1</span></span><br><span class="line"><span class="comment">#2      object      8</span></span><br><span class="line"></span><br><span class="line">target = [<span class="string">'y'</span>]</span><br><span class="line">columns = df_train.columns</span><br><span class="line">num_feats = df_train.dtypes.index[df_train.dtypes!=<span class="string">'object'</span>]</span><br><span class="line">cate_feats = df_train.dtypes.index[df_train.dtypes==<span class="string">'object'</span>]</span><br><span class="line">num_feats = list(set(num_feats)-set(target))</span><br><span class="line"><span class="comment">#输出所有数值型特征和离散型特征</span></span><br><span class="line">print(<span class="string">'numerical features:'</span>)</span><br><span class="line">print(num_feats)</span><br><span class="line">print(<span class="string">'categorical features:'</span>)</span><br><span class="line">print(cate_feats)</span><br><span class="line">print(<span class="string">'target column:'</span>)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#统计缺失值</span></span><br><span class="line"><span class="comment"># check if there are any missing values</span></span><br><span class="line">print(<span class="string">'the following numerical features contains null value'</span>)</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> num_feats:</span><br><span class="line">    <span class="keyword">if</span>(np.sum(df_train[column].isnull())!=<span class="number">0</span>):</span><br><span class="line">        print(column)</span><br><span class="line">print(<span class="string">'the following categorical features contains null value'</span>)</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> cate_feats:</span><br><span class="line">    <span class="keyword">if</span>(np.sum(df_train[column].isnull()!=<span class="number">0</span>)):</span><br><span class="line">        print(column)</span><br><span class="line"><span class="comment"># fill the missing values with mean or median , all depends on the distribution of the corresponding variable</span></span><br><span class="line">missing_df = df_train.isnull().sum(axis=<span class="number">0</span>).reset_index()</span><br><span class="line">missing_df.columns = [<span class="string">'column_name'</span>, <span class="string">'missing_count'</span>]</span><br><span class="line">missing_df = missing_df.ix[missing_df[<span class="string">'missing_count'</span>]&gt;<span class="number">0</span>]</span><br><span class="line">missing_df = missing_df.sort_values(by=<span class="string">'missing_count'</span>)</span><br><span class="line">missing_df</span><br></pre></td></tr></table></figure></li><li><p>目标变量的分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.figure(figsize = (<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">tar_val = np.sort(df_train[target].values)</span><br><span class="line">sns.distplot(tar_val,bins=<span class="number">50</span>,kde=<span class="keyword">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'target value'</span>,fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p>如果目标变量为连续值且其值域范围较大，可以考虑对其做对数变换，并用变换后的值建模。<br>如果目标变量为离散值且正负样本不均衡，可以考虑下采样或这上采样，可以采用对不平衡数据友好的auc等指标指导模型选择，而且再划分训练集和验证集的使用要注意各个类的样本再训练集或验证集中要分布均匀，可以使用sklearn中的StratifiedKFold函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">kf = StratifiedKFold(n_splits = <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> kf.split(train_X):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>各个特征变量的分布</p><p>   特征变量为连续值：如果为长尾分布并且考虑使用线性模型，需要对变量进行对数变换</p><p>   特征变量为离散值：观察每个离散值的频率分布，对出现频率较低的变量统一归为“其他类”，之后进行one-hot 编码或采用数值编码<br>   数值编码可以采用LabelEncoder，one-hot encoder也有对应的函数</p><p>   如何处理缺失值？<br>   特征为连续值：如果分布为偏正态分布，可以用均值填充，如果为长尾分布，一般用中值，避免受outlier的影响。<br>   特征为离散值：用众数填充。</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">lbl_enc = LabelEncoder()</span><br><span class="line">lbl_enc.fit(list(df_train[column_name].values))</span><br><span class="line">df_train[column_name] = lbl_enc.transform(list(df_train[column_name].values))</span><br><span class="line"><span class="comment"># 貌似如果要使用OneHotEncoder函数需要保证column的值是数值，所以需要先用labelencoder预处理一下</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">ohe = OneHotEncoder()</span><br><span class="line">ohe.fit(df_train[column_name])</span><br><span class="line">ohe_features = ohe.transform(df_train[column_name])</span><br></pre></td></tr></table></figure><p>   我一般使用pandas内置的函数做one-hot编码：</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...one hot encoding of categorical variables</span></span><br><span class="line">categorical =  [<span class="string">"X0"</span>, <span class="string">"X1"</span>, <span class="string">"X2"</span>, <span class="string">"X3"</span>, <span class="string">"X4"</span>, <span class="string">"X5"</span>, <span class="string">"X6"</span>, <span class="string">"X8"</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> categorical:</span><br><span class="line">    dummies = pd.get_dummies(all_df[f], prefix = f, prefix_sep = <span class="string">'_'</span>)</span><br><span class="line">    all_df = pd.concat([all_df, dummies], axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>各个特征与标号地分布情况<br>对于categorical features 一般采用box-plot分析，对于数值型特征一般采用scatter或者直接plot<br>box-plot 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var_name = <span class="string">"X6"</span></span><br><span class="line">col_order = np.sort(df_train[var_name].unique()).tolist()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">sns.boxplot(x=var_name, y=<span class="string">'y'</span>, data=df_train, order=col_order)</span><br><span class="line">plt.xlabel(var_name, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">"Distribution of y variable with "</span>+var_name, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>特征两两之间的分布与相关度<br>可以有助于发现高相关和共线性的特征</p></li></ol><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><ol><li>合并文件</li><li>缺失值处理</li><li>离群点处理</li><li>特征编码</li></ol><p>这个部分其实基本上在第一部分数据探索与预处理中已经提到了，主要是因为数据探索主要也是探索需要处理的特征，所以数据清洗与数据探索与预处理是有很大一部分耦合的。</p><h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>坊间传言，特征决定了效果的上限，而算法只是再不断地逼近这个上限，这充分说明了特征在数据挖掘比赛中的重要性。<br>特征工程按照我自己的理解主要是与领域知识相关的，我觉得可以把它分为三个部分：</p><ol><li>特征构建<br>特征构建就是根据原始的数据构建出可以用来训练模型的数据，这主要与参赛者的领域知识密切相关，比如在自然语言处理中常用的tf-idf特征，参赛者对相关领域的了解程度直接决定了构建的特征好与坏。</li><li><p>特征抽取<br>一般是指用PCA,LDA,ICA,SVD等方法自动抽取出有物理意义的特征，这些抽取出的特征一般都是直接与原始特征stacking到一起，而不是要舍弃原始特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_component = <span class="number">12</span>)</span><br><span class="line">pca.fit(df_train)</span><br><span class="line">pca_comp = pca.transform(df_train)</span><br></pre></td></tr></table></figure></li><li><p>特征选择<br>Lasso，ElasticNet等模型能起到特征选择的作用，只保留重要的特征，而基于决策树的一些方法如RandomForest,xgboost等也能够根据构建决策树的过程中各个特征的贡献度给出各个特征的打分，可以根据打分判断各个特征的重要性。当然，也可以使用卡方检验来对特征的重要度排序。根据特征的重要程度选择相对有用的特征用于建模。</p></li></ol><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>对于稀疏型数据（如文本特征，one-hot的ID特征），一般采用线性模型。 Random Forest 和 GBDT等树模型不太适用于稀疏的特征，但可以先对特征降维(PCA,LDA,SVD/LSA)，再使用这些特征。稀疏数据直接输入DNN会导致网络weight过多，不利于优化， 也可以考虑先降维，或者对ID类特征只用embedding的方式。</p><p>对于稠密型特征，一般使用xgboost建模</p><p>数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏数据进行建模，将其输出与稠密特征一起再输入到XGboost等模型中。</p><p>如果要使用线性模型，一般要先标准化输入特征，因为各个特征的尺度可能不一样，标准化能归一化到同一尺度，从而避免某个特征占主导地位，而且对于采用梯度下降优化算法的算法还能加速收敛。</p><p>kaggle大神Abhishek在他的博客中给出了常用的数据挖掘算法并给出了各个算法需要调整的参数及其范围：</p><p>Classification:</p><p>Random Forest，<br>GBM，<br>Logistic Regression，<br>Naive Bayes，<br>Support Vector Machines，<br>k-Nearest Neighbors</p><p>Regression：</p><p>Random Forest，<br>GBM，<br>Linear Regression，<br>Ridge，<br>Lasso，<br>SVR</p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/AAEAAQAAAAAAAAilAAAAJDhiNGE5YTVmLTJhNTAtNGNkZS1hNDAwLTY5YTJiMTE1ZmI3Zg.png" alt=""></p><h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><ol><li>Voting 和 Averaging<br>直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。</li><li>Stacking<br>这里重点介绍stacking</li></ol><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/stacking.jpg" alt=""></p><p>stacking与bagging和boosting一样，都属于集成学习的一种，bagging是一种并行的集成学习模式，boosting是一种串行的集成学习模式，而stacking则不同于这两种方法，bagging和boosting都是只有一层的集成学习，而stacking则可以有多层，stacking通过将前一层的输出输入到下一层中，从而组成了一种多层模型的集成学习模式。如上图所示，假设现在要训练第一层模型，具体步骤如下：</p><pre><code>1. 首先将训练集分成5-folds2. 在其中4-folds上训练，给出在剩下的一个fold 上的预测值，同时给出测试集的预测值。3. 重复步骤2，5次。4. 将步骤2中5次的验证集上的预测值组合成一个长的向量，长度为训练集的大小，这个向量就是该算法输入到下一层的输入，同时对步骤2中五次的测试集预测值求平均5. 换另一个算法重复步骤1到步骤46. 将所有算法得到的训练集预测值按列组合成一个高为训练集大小，宽度为算法个数的矩阵，该矩阵就是输入到下一层的样本，每一行为一个样本，每一列为一个特征，同时对于测试集上的预测值也组合成一个矩阵，输入到下一层中当测试集。（貌似挺拗口）</code></pre><p>上面是一种stacking的方法，可以看出每层算法的个数决定了下一层输入样本的特征数，所以算法少的话，下一层的输入特征就变得少了，所以stacking还有其他的变种，其中一种是将上一层的输入与上一层的输出直接按列组合成一个新的矩阵输入到下一层中，比如现在有输入样本 100$\times$300,如果第一层只有一个算法，那么输入到第二层的样本矩阵大小为  100$\times$301，有效避免了特征过少的问题。 结合我们之前所说的，对于稀疏特征常采用线性模型，对于稠密特征采用复杂一点的模型比如xgboost,如果既有稀疏特征又有稠密特征，可以在第一层先使用线性模型对稀疏特征建模，将其输出在第二层与稠密特征组合形成第二层的输入，并在第二层采用xgboost等模型。</p><h3 id="自动化框架"><a href="#自动化框架" class="headerlink" title="自动化框架"></a>自动化框架</h3><p>之前我也曾试过kaggle上的一些入门竞赛，但是没有那么认真做，写完代码，提交完结果之后就没怎么关注过了，所以我的代码没有系统性，也没有做好整理，所以，这次参加比赛的代码又从头开始写了。 。从上面的介绍可以看出，写好一套代码，是可以在多个数据集上重用的，而且，在做特征工程的时候，常常需要不时地添加或删除特征，所以输入数据也在一直变化，如果能够将后面地几个步骤如模型选择，参数调优，模型融合写成一个自动化地框架，那就非常方便了，因为不用每次都要手动调节参数，手动记录结果，大大地节省了时间也提高了效率。目前我主要参考的是kaggle大神 Chenglong Chen在 <a href="https://github.com/Matafight/Kaggle_CrowdFlower" target="_blank" rel="noopener">Crowdflower</a>竞赛中开源的代码，他的代码在模型融合中使用了bagged ensemble selection, 来自于一篇ICML2004的论文<a href="https://scholar.google.com.hk/scholar?q=Ensemble+selection+from+libraries+of+models&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Ensemble<br>selection from libraries of models</a>。另一个同样来自Chenglong Chen的开源代码<a href="https://github.com/Matafight/Kaggle_HomeDepot" target="_blank" rel="noopener">HomeDepot</a>中采用了stacking的模型融合方式，看来单一的模型融合方式不能适用于多种任务，还是要针对具体任务具体分析，多尝试，从而得到相对好的结果。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur" target="_blank" rel="noopener">https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur</a><br><a href="https://zhuanlan.zhihu.com/p/26820998" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26820998</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近参加了kaggle的一个比赛，看了好多关于数据处理，特征分析以及集成学习的一些内容，这些都是数据挖掘大神总结出来的经验，在这里总结一下我的收获，以及谈一下我自己的一些想法。&lt;/p&gt;
&lt;p&gt;参加数据挖掘比赛一般要从以下几个方面着手：&lt;/p&gt;
&lt;h3 id=&quot;数据探索与预处
      
    
    </summary>
    
      <category term="kaggle" scheme="http://matafight.github.io/categories/kaggle/"/>
    
    
      <category term="kaggle" scheme="http://matafight.github.io/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>Cython简单介绍</title>
    <link href="http://matafight.github.io/2017/05/12/cython-introduction-md/"/>
    <id>http://matafight.github.io/2017/05/12/cython-introduction-md/</id>
    <published>2017-05-12T07:01:33.000Z</published>
    <updated>2017-12-12T13:56:25.531Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>前一段时间想自己实现并改造因子分解机，libfm的源码是用C++写的，虽然高效但是代码的可读性较差，而python虽然有较好的可读性，而且python有方便的科学计算工具包，但是python本身的运行效率不太高。幸运的是，由于python的胶水语言的特点，有很多将python与C等相对效率高的语言结合起来的方法，由python做前端计算，而C等做后台计算。既实现了提升速度的目的，写起来又简单。</p><p>Cython 全称是 C-extension for python,是一种用于编译python和cython语法的编译器。cython的语法与python很相似，Cython的编译器能够将用Cython语法编写的代码转换成C代码，然后执行高效的C代码。用户只需编写类似python语法的Cython代码就可以。</p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p><strong>基本流程</strong><br>Cython文件的后缀为.pyx, 该文件先由Cython编译器编译为C文件，之后C文件再由系统的C编译器编译成.so文件(windows平台是.pyd文件)，之后python就能import该模块了.<br><strong>安装与导入</strong><br>python中通常使用distutils来构建或安装新的模块，这需要编写setup.py文件。文件的内容挺简单，一个例子如下，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line">setup(</span><br><span class="line">    name = <span class="string">'higher-order-fm'</span>,</span><br><span class="line">    ext_modules = cythonize(<span class="string">'higher_fm_cython.pyx'</span>),</span><br><span class="line">    include_dirs=[numpy.get_include()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发布模块</span></span><br><span class="line">python setup.py build_ext --inplace</span><br><span class="line"><span class="comment">#之后可以使用 import class_name 在python代码中使用模块，class_name是在Pyx文件中定义的类名。</span></span><br></pre></td></tr></table></figure><p><strong>其他注意事项</strong><br>函数定义可以采用 def和cdef两种形式，采用cdef定义的函数对于导入 在pyx中定义的 模块的python代码是不可见的，只有def定义的函数可见！！而且cdef定义的函数内所有的变量都要显示声明其类型，看上去在写python，但实际上是在写c。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;前一段时间想自己实现并改造因子分解机，libfm的源码是用C++写的，虽然高效但是代码的可读性较差，而python虽然有较好的可读性，而且p
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Cython" scheme="http://matafight.github.io/tags/Cython/"/>
    
  </entry>
  
  <entry>
    <title>kaggle 入门</title>
    <link href="http://matafight.github.io/2017/03/23/kaggle-%E5%85%A5%E9%97%A8/"/>
    <id>http://matafight.github.io/2017/03/23/kaggle-入门/</id>
    <published>2017-03-23T09:10:17.000Z</published>
    <updated>2017-12-12T13:56:25.539Z</updated>
    
    <content type="html"><![CDATA[<p><strong>这里以Kaggle上的一个入门竞赛<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">House Prices</a> 为例</strong></p><h3 id="处理缺失数据和非数值特征"><a href="#处理缺失数据和非数值特征" class="headerlink" title="处理缺失数据和非数值特征"></a>处理缺失数据和非数值特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读入数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">test_ids=  test_data[<span class="string">'Id'</span>]</span><br><span class="line"><span class="comment">#将训练集和测试集一同处理，可以利用train_data.head()方法查看数据前几行，而且还可以查看dataframe的columns names,从而可以选择指定特征列，如下行所示。</span></span><br><span class="line">all_data = pd.concat((train_data.loc[:,<span class="string">'MSSubClass'</span>:<span class="string">'SaleCondition'</span>],test_data.loc[:,<span class="string">'MSSubClass'</span>:<span class="string">'SaleCondition'</span>]),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看所有数值特征</span></span><br><span class="line">num_feat = all_data.dtypes[all_data.dtypes !=<span class="string">'object'</span>].index</span><br><span class="line"><span class="comment">#用均值填充空值</span></span><br><span class="line">all_data[num_feat] = all_data[num_feat].fillna(all_data[num_feat].mean())</span><br><span class="line"><span class="comment">#对于非数值特征，使用one-hot-encoding方式转换，对于值为null的非数值特征，转换后为全零向量</span></span><br><span class="line">all_data = pd.get_dummies(all_data)</span><br></pre></td></tr></table></figure><h3 id="进一步处理"><a href="#进一步处理" class="headerlink" title="进一步处理"></a>进一步处理</h3><p>查看目标值分布：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.distplot(train_data[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure></p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/1.PNG" alt=""><br>可以看到分布的中心向左偏斜,对其取对数得到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(np.log1p(train_data[<span class="string">'SalePrice'</span>]))</span><br></pre></td></tr></table></figure></p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/2.PNG" alt=""><br>经过对数转换的数值明显对称了许多，对称与不对称这个指标可以使用 scipy.stats 中的skew函数来计算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> skew</span><br><span class="line">print(skew(train_data[<span class="string">'SalePrice'</span>]))</span><br><span class="line">print(skew(np.log1p(train_data[<span class="string">'SalePrice'</span>])))</span><br><span class="line"><span class="comment">#输出分别为1.8809 0.1212，数值越大，偏斜越大</span></span><br><span class="line">train_data[<span class="string">'SalePrice'</span>] = np.log1p(train_data[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure></p><p>同理，偏斜指标也可以应用到特征中，也可以对偏斜的特征做对数变换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">skew_val = all_data[num_feat].apply(<span class="keyword">lambda</span> x:skew(x))</span><br><span class="line">skew_feat = skew_val[skew_val &gt; <span class="number">0.75</span>].index <span class="comment">#计算偏度大于给定阈值的特征,对这些特征做对数变换</span></span><br><span class="line">all_data[skew_feat] = np.log1p(all_data[skew_feat])</span><br><span class="line">x_train = all_data.iloc[:train_data.shape[<span class="number">0</span>],:]</span><br><span class="line">y_train = train_data[<span class="string">'SalePrice'</span>]</span><br><span class="line">x_test = all_data.iloc[train_data.shape[<span class="number">0</span>]:,:]</span><br></pre></td></tr></table></figure></p><h3 id="选择模型训练与预测"><a href="#选择模型训练与预测" class="headerlink" title="选择模型训练与预测"></a>选择模型训练与预测</h3><p>这里使用adaboost模型来训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> ParameterGrid</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span>  mean_squared_error</span><br><span class="line">kfold = KFold(x_train.shape[<span class="number">0</span>],<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">param_grid = &#123;<span class="string">'silent'</span>:[<span class="number">1</span>],</span><br><span class="line">              <span class="string">'nthread'</span>:[<span class="number">2</span>],</span><br><span class="line">              <span class="string">'eval_metric'</span>:[<span class="string">'rmse'</span>],</span><br><span class="line">              <span class="string">'eta'</span>:[<span class="number">0.03</span>],</span><br><span class="line">              <span class="string">'objective'</span>:[<span class="string">'reg:linear'</span>],</span><br><span class="line">              <span class="string">'max_depth'</span>:[<span class="number">5</span>,<span class="number">7</span>],</span><br><span class="line">              <span class="string">'num_round'</span>:[<span class="number">500</span>],</span><br><span class="line">              <span class="string">'subsample'</span>:[<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>],</span><br><span class="line">              <span class="string">'colsample_bytree'</span>:[<span class="number">0.5</span>,<span class="number">0.7</span>]&#125;</span><br><span class="line"></span><br><span class="line">min_score = <span class="number">10000</span></span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> ParameterGrid(param_grid):</span><br><span class="line">    print(params)</span><br><span class="line">    total_score = []</span><br><span class="line">    <span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> kfold:</span><br><span class="line">        train_x_train,test_x_train = x_train.iloc[train_index,:],x_train.iloc[test_index,:]</span><br><span class="line">        train_y_train,test_y_train = y_train[train_index],y_train[test_index]</span><br><span class="line">        xgb_train = xgb.DMatrix(train_x_train,label = train_y_train)</span><br><span class="line">        xgb_test =  xgb.DMatrix(test_x_train,label = test_y_train)</span><br><span class="line">        watchlist = [(<span class="string">'train'</span>,xgb_train),(<span class="string">'test'</span>,xgb_test)]</span><br><span class="line">        model = xgb.train(params,<span class="number">200</span>,watchlist,early_stopping_rounds = <span class="number">20</span>)</span><br><span class="line">        y_preds = model.predict(xgb_test)</span><br><span class="line">        total_score.append(np.sqrt(mean_squared_error(y_preds,test_y_train)))</span><br><span class="line">    <span class="keyword">if</span>(min_score &gt; np.mean(total_score)):</span><br><span class="line">        min_score = np.mean(total_score)</span><br><span class="line">        best_params = params</span><br><span class="line">        best_rounds=  model.best_iteration</span><br><span class="line"><span class="comment">#0.1345</span></span><br><span class="line"></span><br><span class="line">xgb_test_data = xgb.DMatrix(x_test)</span><br><span class="line">xgb_train_data = xgb.DMatrix(x_train,label = y_train)</span><br><span class="line">wathchlist=  [(<span class="string">'train'</span>,xgb_train_data)]</span><br><span class="line">new_model = xgb.train(best_params,xgb_train_data,best_rounds,watchlist)</span><br><span class="line">pred_test = np.expm1(new_model.predict(xgb_test_data))</span><br><span class="line"></span><br><span class="line"><span class="comment">#submission</span></span><br><span class="line">summ = pd.DataFrame(&#123;<span class="string">'Id'</span>:test_ids,<span class="string">'SalePrice'</span>:pred_test&#125;)</span><br><span class="line">summ.to_csv(<span class="string">'mysumm_adaboost.csv'</span>,index = <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/3.PNG" alt=""><br>上图是我提交的结果，下面比较好的结果是直接运行讨论版里一个使用<a href="https://www.kaggle.com/guosch/house-prices-advanced-regression-techniques/regularized-linear-models" target="_blank" rel="noopener">Lasso做出来的结果</a>，0.13的结果是使用上面adaboost代码跑出来的结果，数据预处理两个方法是类似的，问题在于adaboost的参数可能没调好，而且adaboost可能过拟合了。</p><p>下一步，研究<a href="https://www.kaggle.com/c/crowdflower-search-relevance" target="_blank" rel="noopener">Crowdflower Search Results Relevance</a>竞赛第一名的<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower" target="_blank" rel="noopener">解决方案</a>.<br>以及<a href="https://zhuanlan.zhihu.com/p/25742261" target="_blank" rel="noopener">Kaggle 入门指南</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;这里以Kaggle上的一个入门竞赛&lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ho
      
    
    </summary>
    
      <category term="kaggle" scheme="http://matafight.github.io/categories/kaggle/"/>
    
    
      <category term="kaggle" scheme="http://matafight.github.io/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>xgboost 常用参数</title>
    <link href="http://matafight.github.io/2017/03/23/xgboost%20%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0/"/>
    <id>http://matafight.github.io/2017/03/23/xgboost 常用参数/</id>
    <published>2017-03-23T09:08:31.000Z</published>
    <updated>2017-12-12T13:56:25.567Z</updated>
    
    <content type="html"><![CDATA[<h3 id="XGBoost-的参数"><a href="#XGBoost-的参数" class="headerlink" title="XGBoost 的参数"></a><a href="http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters" target="_blank" rel="noopener">XGBoost 的参数</a></h3><h4 id="一般参数"><a href="#一般参数" class="headerlink" title="一般参数"></a>一般参数</h4><ol><li>booster[default = gbtree],使用哪种基分类器，可选：gblinear,dart</li><li>silent[default = 0],0表示输出运行信息，1表示不输出</li><li>nthread, 使用多少线程运行xgboost</li><li>num_pbuffer,xgboost自动设置</li><li>num_feature,自动设置，取决于读入样本特征的最大值</li></ol><h4 id="Tree-booster的参数"><a href="#Tree-booster的参数" class="headerlink" title="Tree booster的参数"></a>Tree booster的参数</h4><ol><li><strong>eta[default = 0.3,alias: learn_rate]</strong>,缩减系数，范围[0-1]</li><li><strong>gamma[default = 0,alias:min_split_loss]</strong><br> 分裂节点的损失减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，所以要平衡</li><li><strong>max_depth[default = 6],</strong>树的最大深度，增大深度容易过拟合</li><li><strong>min_child_weight[default = 1]</strong>,最小样本权重和（每个样本的权重等于其对应的二阶导），如果新分裂的节点的样本权重和小于min_child_weight则停止分裂。range:[0,无穷大]</li><li><strong>max_delta_step[default = 0]</strong>,待定</li><li><strong>subsample[default = 1]</strong>,每棵树对训练集的采样率，如果设为0.5，则每训练一棵树都随机选择一半的训练集训练。</li><li><strong>colsample_bytree[default = 1]</strong>,刚开始构建树时对特征的采样率</li><li><strong>colsample_bylevel[default = 1]</strong>,在构建每一层节点时列的采样率</li><li><strong>lambda[default = 1]</strong>,L2范数正则项系数</li><li><strong>alpha[default = 0]</strong>,L1正则项系数</li><li><strong>tree_method,string[default=’auto’]</strong><br>choices:{‘auto’,’exact’,’approx’}</li><li><strong>sketch_eps[default = 0.03]</strong>,近似直方图算法中分桶的参数，仅用于近似算法中，一般不用设定</li><li><strong>scale_pos_weight[default = 1]</strong>,用于控制正例与负例权重的平衡，对于不平衡数据有用，<a href="http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html" target="_blank" rel="noopener">具体参见这里</a></li></ol><h4 id="Learning-Task-parameters"><a href="#Learning-Task-parameters" class="headerlink" title="Learning Task parameters"></a>Learning Task parameters</h4><p>学习任务参数：objective[default=reg:linear]</p><ol><li>“reg:linear”,linear regression</li><li>“reg:logistic”,logistic regression</li><li>“binary:logistic”,用于二类问题的logistic regression,输出概率</li><li>“binary:logitraw”,同上，不过输出的是sigmoid function转换之前的score</li><li>“count:poisson”,用于计数问题的泊松回归问题？</li><li>“multi:softmax”,用于多类分类问题，并采用softmax损失函数，需要设定 num_class</li><li>“multi:softprob”,同上，不过输出所有n个类的概率</li><li>“rank:pairwise”,用于排序问题，损失函数为pairwise loss</li><li>“reg::gamma”</li><li>base_score[default = 0.5],所有样本的初始预测值，迭代次数多时，该参数影响不大</li><li>eval_metric,评价准则<br>有以下几种：<pre><code>1. &apos;rmse&apos;2. &apos;mae&apos;3. &apos;logloss&apos;,  负的对数似然4. &apos;error&apos;,二类分类错误率，输出大于0.5时为正例，否则负例5. &apos;error@t&apos;,同上，不过指定正负例阈值6. &apos;merror&apos;,多类分类错误率7. &apos;mlogloss&apos;,多类的logloss8. &apos;auc&apos;9. &apos;ndcg&apos;,[Normalized Discounted Cumulative Gain][3]10. &apos;map&apos;,[Mean average precision][4]11. 等。。</code></pre></li><li>seed[default = 0]</li><li>random number seed</li></ol><p><a href="https://github.com/dmlc/xgboost/blob/master/demo/README.md#basic-examples-by-tasks" target="_blank" rel="noopener">XGBoost一些样例代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;XGBoost-的参数&quot;&gt;&lt;a href=&quot;#XGBoost-的参数&quot; class=&quot;headerlink&quot; title=&quot;XGBoost 的参数&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://xgboost.readthedocs.io/en/latest/para
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="XGBoost" scheme="http://matafight.github.io/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost 笔记</title>
    <link href="http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/"/>
    <id>http://matafight.github.io/2017/03/14/XGBoost-简介/</id>
    <published>2017-03-14T12:49:24.000Z</published>
    <updated>2017-12-12T13:56:25.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  XGBoost是一种基于决策树（CART）的分布式的高效的梯度提升算法，它可被应用到分类、回归、排序等任务中，与一般的GBDT算法相比，XGBoost主要有以下几个优点：</p><ol><li>对叶节点的权重进行了惩罚，相当于添加了正则项，防止过拟合</li><li>XGBoost的目标函数优化利用了损失函数关于待求函数的二阶导数，而GBDT只利用了一阶信息</li><li>XGBoost支持列采样，类似于随机森林，构建每棵树时对属性进行采样，训练速度快，效果好</li><li>类似于学习率，学习到一棵树后，对其权重进行缩减，从而降低该棵树的作用，提升可学习空间</li><li>构建树的算法包括精确的算法和近似的算法，近似的算法对每维特征加权分位进行分桶，具体的算法利用到了损失函数关于待求树的二阶导数。</li><li>添加了对于稀疏数据的支持，当数据的某个特征缺失时，将该数据划分到默认的子节点，本文提出了一个算法来求解这个默认方向。</li><li>可并行的近似直方图算法，分裂节点时，数据在block中按列存放，而且已经经过了预排序，因此可以并行计算，即同时对各个属性遍历最优分裂点</li></ol><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>  boosting是属于串行的集成方法，其预测函数为多个基分类器的集成，其学习过程也是先学习前(t-1)个基分类器，再学习第t个基分类器。XGBoost中最主要的基学习器为CART（分类与回归树），因此，其预测函数为:<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_1.PNG" alt=""><br>  其中$K$表示有$K$个决策树<img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_2.PNG" alt="">表示决策树空间，$q(x)$表示将输入样本$x$映射到树的叶子节点$q(x)$，其对应叶子节点的标号为$\omega_{q(x)}$。<br>  因此，正则化的目标函数可以写成：<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_3.PNG" alt=""><br>  上述目标函数的参数是决策树，因此不能在一般的欧式空间优化，这里类似于adaboost算法的优化过程，也是利用加性模型的特点来训练。假设前$t-1$步迭代优化得到的模型为$f^{t-1}(x)$,在第$t$步中，待求参数为$f^t$，则第$t$步的目标函数为:<br>  <img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_4.PNG" alt=""><br>将上式进行二阶泰勒展开可以得到：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_5.PNG" alt=""><br>其中<img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_6.PNG" alt=""><br>去掉与待求参数无关的常数项，从而得到新的优化目标为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_7.PNG" alt=""><br>将上式变形，将关于样本迭代转换为关于树的叶子节点迭代：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_8.PNG" alt=""></p><p>这样，对于给定的树结构，可以直接计算得到叶子节点的标号：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_9.PNG" alt=""><br>以及相对应的最优目标函数值：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_10.PNG" alt=""></p><p>但是,由于树的结构是未知的，而且也不可能去遍历所有的树结构。因此，本文采用贪婪算法来分裂节点，从根节点开始，遍历所有属性，遍历属性的可能取值，记分到左子树的样本集为$I_L$,分到右子树的样本集为$I_R$，则分裂该节点导致的损失减少值为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_11.PNG" alt=""><br>我们希望找到一个属性以及其对应的大小，使得上式取值最大。</p><h2 id="分裂节点算法-SPLIT-FINDING-ALGORITHMS"><a href="#分裂节点算法-SPLIT-FINDING-ALGORITHMS" class="headerlink" title="分裂节点算法(SPLIT FINDING ALGORITHMS)"></a>分裂节点算法(SPLIT FINDING ALGORITHMS)</h2><p>因为树结构未知，只能采用贪婪的算法，从根节点出发，每次选择一个属性及其对应的值，使得损失函数减少最多，根据选择的属性分裂节点。论文中给出了精确的和近似的算法，当数据量非常大时，采用近似的算法可以有效减少计算量。</p><h3 id="精确贪婪算法-Basic-Exact-Greedy-Algorithms"><a href="#精确贪婪算法-Basic-Exact-Greedy-Algorithms" class="headerlink" title="精确贪婪算法(Basic Exact Greedy Algorithms)"></a>精确贪婪算法(Basic Exact Greedy Algorithms)</h3><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_12.PNG" alt=""><br>这里的$m$值通常小于样本维度$d$,表示列采样得到的属性个数，值得注意的是，由于要遍历所有的属性的所有取值，因此，通常需要在训练之前对所有样本做一个预排序(pre-sort)，从而避免每次选择属性都要重新排序。</p><h3 id="近似算法-Approximate-Algorithm-for-Split-Finding"><a href="#近似算法-Approximate-Algorithm-for-Split-Finding" class="headerlink" title="近似算法(Approximate Algorithm for Split Finding)"></a>近似算法(Approximate Algorithm for Split Finding)</h3><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_13.PNG" alt=""><br>对于值为连续值的特征，当样本数非常大时，该特征取值过多，遍历所有取值复杂度较高，而且容易过拟合。因此，考虑将特征值分桶，即找到$l$个分位点，将位于相邻分位点之间的样本分在一个桶中，在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。注意到上面算法流程中说明了有全局的近似(global)和局部(local)的近似，所谓全局就是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部就是在具体的某一次分裂节点的过程中采用近似算法。</p><h3 id="Weighted-Quantile-Sketch"><a href="#Weighted-Quantile-Sketch" class="headerlink" title="Weighted Quantile Sketch"></a>Weighted Quantile Sketch</h3><p><strong>带权重直方图算法</strong><br>主要用于近似算法中分位点的计算，假设分位点为 ${s<em>{k1},s</em>{k2},..,s<em>{kl}}$，假设 $(x</em>{1k},h<em>1),(x</em>{2k},h<em>2),..,(x</em>{nk},h_n)$ 表示所有样本的第 $k$ 个特征值及二阶导数，定义一个rank function 如下：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_14.PNG" alt=""><br>希望得到的分位点满足如下条件：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_15.PNG" alt=""><br>这意味着大概有$\frac{1}{\epsilon}$个分位点。</p><p><strong>为什么用二阶导数作为权重？</strong><br><em>因为目标函数还可以写成带权的形式：</em><br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_16.PNG" alt=""><br>上式相当于带权重的损失函数，权重为$h_i$。<br>为了优化该问题，本文还提出了分布式 <em>weighted quantile sketch algorithm</em> ，该算法的优点是解决了带权重的直方图算法问题，以及有理论保证。</p><h3 id="Sparsity-aware-Split-Finding"><a href="#Sparsity-aware-Split-Finding" class="headerlink" title="Sparsity-aware Split Finding"></a>Sparsity-aware Split Finding</h3><p>XGBoost还特别设计了针对稀疏数据的算法，假设样本的第$i$个特征缺失时，无法利用该特征对样本进行划分，这里的做法是将该样本默认地分到指定的子节点，至于具体地分到哪个节点还需要下面的算法来计算：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/xgboost_17.PNG" alt=""><br>该算法的主要思想是，分别假设特征缺失的样本属于右子树和左子树，而且只在不缺失的样本上迭代，分别计算缺失样本属于右子树和左子树的增益，选择增益最大的方向为缺失数据的默认方向。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="Column-Block-for-Parallel-Learning"><a href="#Column-Block-for-Parallel-Learning" class="headerlink" title="Column Block for Parallel Learning"></a>Column Block for Parallel Learning</h3><p>算法中最耗时的部分就是预排序，为了节省排序的时间，XGBoost将数据存在内存单元block中(??)，同时在block采用CSC 格式存放(Compressed Column format)，每一列(一个属性列)均升序存放，这样，一次读入数据并排好序后，以后均可使用。在精确贪心算法中，将所有数据均导入内存，算法只要在数据中线性扫描已经预排序过的特征就可以。对于近似算法，可以用多个block(Multiple blocks)分别存储不同的样本集，多个block可以并行计算，<br>重要的是，由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行split finding算法，从而并行化split finding.</p><h3 id="Cache-aware-Access"><a href="#Cache-aware-Access" class="headerlink" title="Cache-aware Access"></a>Cache-aware Access</h3><p>由于样本按特征进行了预排序，样本对应的统计量(一阶和二阶梯度)需要根据行索引来查找，这导致了内存的不连续访问，容易导致cpu cache命中率降低。—这块没太搞懂</p><h3 id="Blocks-for-Out-of-core-Computation"><a href="#Blocks-for-Out-of-core-Computation" class="headerlink" title="Blocks for Out-of-core Computation"></a>Blocks for Out-of-core Computation</h3><p>为了更好地利用计算机的磁盘空间，对于不能一次性导入到内存的数据，我们将数据分成多个block存在磁盘上，在计算过程中，用另外的线程读取数据，但是由于磁盘IO速度太慢，通常更不上计算的速度。所以，我们采用了下面两种方法有优化速度和存储：</p><ol><li>Block compression<br>将block按列压缩，对于行索引，只保存第一个索引值，然后只保存该数据与第一个索引值之差(offset)，一共用16个bits来保存 offset,因此，一个block一般有$2^{16}$个样本。</li><li>Block sharding<br>提高磁盘的吞吐量(??)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;  XGBoost是一种基于决策树（CART）的分布式的高效的梯度提升算法，它可被应用到分类、回归、排序等任务中，与一般的GBDT算法相比，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://matafight.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="XGBoost" scheme="http://matafight.github.io/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>C++ primer-泛型算法</title>
    <link href="http://matafight.github.io/2017/02/21/C-%E6%B3%9B%E5%9E%8B%E7%AE%97%E6%B3%95/"/>
    <id>http://matafight.github.io/2017/02/21/C-泛型算法/</id>
    <published>2017-02-21T13:23:06.000Z</published>
    <updated>2017-12-12T13:56:25.502Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第十章-泛型算法"><a href="#第十章-泛型算法" class="headerlink" title="第十章 泛型算法"></a>第十章 泛型算法</h1><h2 id="10-1-概述"><a href="#10-1-概述" class="headerlink" title="10.1. 概述"></a>10.1. 概述</h2><p>标准库已经为容器类型定义了一些基本操作，但是并没有为各种容器定义诸如排序，查找特定元素等算法，但是标准库定义了一些泛型算法用来解决诸如容器的排序查找等问题。之所以称之为泛型算法是因为，这些算法针对多种容器的多种数据类型均有效。<br>大多数算法都定义在头文件algorithm 中，一般情况下，<strong>泛型算法并不直接操作容器 </strong>，而是遍历由两个迭代器指定的元素的范围进行操作。<br>迭代器令算法不依赖于容器，但是算法依赖于元素操作的类型，因为算法常常需要比较容器中两个元素的大小，这就要求元素类型支持“&lt;”或”==”运算符，不过大多数算法都可以使用自定义操作来代替运算符。</p><a id="more"></a><p>练习 10.1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line">   <span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">   <span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; a = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">12</span>, <span class="number">2</span> &#125;;<span class="comment">//列表初始化</span></span><br><span class="line">       <span class="keyword">int</span> count_times = <span class="built_in">std</span>::count(a.begin(), a.end(), <span class="number">2</span>);</span><br><span class="line">       <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; count_times &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">       system(<span class="string">"pause"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><pre><code>练习 10.2</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">list</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt; a = &#123; <span class="string">"a"</span>, <span class="string">"bb"</span> ,<span class="string">"a"</span>&#125;;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> val = <span class="string">"a"</span>;</span><br><span class="line"><span class="keyword">int</span> count_times = <span class="built_in">std</span>::count(a.begin(), a.end(), val);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; count_times &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><h2 id="10-2-初识泛型算法"><a href="#10-2-初识泛型算法" class="headerlink" title="10.2 初识泛型算法"></a>10.2 初识泛型算法</h2><h3 id="10-2-1-只读算法"><a href="#10-2-1-只读算法" class="headerlink" title="10.2.1 只读算法"></a>10.2.1 只读算法</h3><p>顾名思义，只读算法只读取迭代器指定范围内的元素，而不改变元素，如 find、count等算法，还有accumulate（求和）算法。<br>练习10.3:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; a = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="built_in">std</span>::accumulate(a.cbegin(), a.cend(), <span class="number">0</span>);<span class="comment">//accumulate定义在头文件 numeric中</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; sum &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p><p>练习 10.4<br>会导致精度损失，相当于将double类型强制转换为int类型，会损失小数点后面的数。</p><h3 id="10-2-2-写容器算法"><a href="#10-2-2-写容器算法" class="headerlink" title="10.2.2 写容器算法"></a>10.2.2 写容器算法</h3><p>一些算法将新值赋予容器中的元素。当我们使用此类算法时，应当注意容器的大小一定不能小于我们要写入的元素的个数。记住因为算法不会执行容器操作，因此它们自身不能改变容器的大小。</p><h4 id="算法不检查写操作"><a href="#算法不检查写操作" class="headerlink" title="算法不检查写操作"></a>算法不检查写操作</h4><p>因此，对一个空容器进行写操作是危险的，如：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec; <span class="comment">//空容器</span></span><br><span class="line">fill(vec.begin(),<span class="number">10</span>,<span class="number">0</span>); <span class="comment">//错误</span></span><br><span class="line">fill(vec.begin(),vec.end(),<span class="number">0</span>);<span class="comment">//安全</span></span><br></pre></td></tr></table></figure></p><h4 id="介绍back-inserter"><a href="#介绍back-inserter" class="headerlink" title="介绍back_inserter"></a>介绍back_inserter</h4><p>一种保证算法有足够的空间来容纳输入数据的方法是使用 <strong>插入迭代器</strong> .插入迭代器是一种向容器中插入元素的迭代器。<br>back_inserter 接受一个指向容器的引用，返回一个与该容器绑定的插入迭代器，通过对该迭代器赋值，赋值操作符会调用push_back 将一个具有给定值的元素添加到容器中。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">it = back_inserter(vec);</span><br><span class="line">*it = <span class="number">42</span>;</span><br></pre></td></tr></table></figure></p><p>我们常常通过back_inserter创建一个迭代器，作为算法的目标位置使用：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">fill_n(back_inserter(vec),<span class="number">10</span>,<span class="number">0</span>);</span><br></pre></td></tr></table></figure></p><h4 id="拷贝算法"><a href="#拷贝算法" class="headerlink" title="拷贝算法"></a>拷贝算法</h4><p>用来将一个容器中的内容拷贝到另一个容器。传递给copy的目标序列至少要包含与输入序列一样多的元素。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> a2[<span class="keyword">sizeof</span>(a)/<span class="keyword">sizeof</span>(*a)];</span><br><span class="line">copy(begin(a),end(a),a2);<span class="comment">//把a的内容拷贝给a2，copy的范围值为指向拷贝到a2尾元素之后的位置</span></span><br></pre></td></tr></table></figure></p><p>上面的代码中值得注意的是sizeof(a)的值等于24，但是sizeof(a+0)却等于4，a是个数组名，它是一个地址常量，但是在某些情况下，比如sizeof(a)，a代表整个数组，而不是这个常量本身，所以数组名并不仅仅是一个指针，而(a+0)相当于指向第零个元素的指针，其大小为4。<br>replace函数能替换指定范围迭代器内的元素值<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">replace(a.begin(),a.end(),<span class="number">0</span>,<span class="number">42</span>);<span class="comment">//将0替换为42，原地替换</span></span><br></pre></td></tr></table></figure></p><p>replace还有一个copy版本<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">replace_copy(a.begin(),a.end(),back_inserter(vec),<span class="number">0</span>,<span class="number">42</span>); <span class="comment">//vec包含容器a的一份拷贝，不过vec中0换成了42</span></span><br></pre></td></tr></table></figure></p><p>练习10.6<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;vec = &#123; <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line"><span class="built_in">std</span>::fill_n(vec.begin(), vec.size(), <span class="number">0</span>);</span><br></pre></td></tr></table></figure></p><p>练习10.7<br>(a)<br>copy(lst.cbegin(),lst.cend(),vec)中，vec是空容器，无法copy,可以使用<br>copy(lst.cbegin(),lst.cend(),back_inserter(vec))<br>或者vec.resize(lst.size())<br>(b)<br>vec.reserve(n)只是预分配内存，但是实际不可访问，只是告诉程序我需要这么大的空间，当n&gt;capacity时，才会重新分配空间，当n&lt;= capacity时，则不会重新分配。 size与capacity的区别: size是容器中的元素数量，capacity指在这段连续的空间中，容器最多可以存储的数量。<br>可以用resize()代替reserve()<br>(c)算法并没有改变容器的大小，而是通过插入迭代器改变的。</p><h3 id="10-2-3-重排容器元素的算法"><a href="#10-2-3-重排容器元素的算法" class="headerlink" title="10.2.3 重排容器元素的算法"></a>10.2.3 重排容器元素的算法</h3><h4 id="消除重复单词"><a href="#消除重复单词" class="headerlink" title="消除重复单词"></a>消除重复单词</h4><p>习题10.9<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;numeric&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">elimDup</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; &amp;words)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">sort(words.begin(), words.end());</span><br><span class="line"><span class="keyword">auto</span> end_pos = unique(words.begin(), words.end());<span class="comment">//unique将不重复的元素排在前面，返回第一个被重复的元素的位置</span></span><br><span class="line">words.erase(end_pos, words.end());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; words = &#123; <span class="string">"the"</span>, <span class="string">"quick"</span>, <span class="string">"red"</span>, <span class="string">"fox"</span>, <span class="string">"jumps"</span>, <span class="string">"over"</span>, <span class="string">"the"</span>, <span class="string">"slow"</span>, <span class="string">"red"</span>, <span class="string">"turtle"</span> &#125;;</span><br><span class="line">elimDup(words);</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;::iterator iter = words.begin(); iter != words.end(); iter++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt; (*iter) &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//cout&lt;&lt;string，需要包含头文件&lt;string&gt;</span></span><br><span class="line">&#125;</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>练习10.10,为什么算法不改变容器大小？<br>因为要将算法操作与容器操作分离，便于设计。<br>算法通过迭代器改变容器大小。</p><h3 id="10-3-定制操作"><a href="#10-3-定制操作" class="headerlink" title="10.3 定制操作"></a>10.3 定制操作</h3><p>很多算法需要比较元素，标准库允许我们使用自定义的比较函数等替代运算符&lt;,==等。</p><h4 id="10-3-1向算法传递函数"><a href="#10-3-1向算法传递函数" class="headerlink" title="10.3.1向算法传递函数"></a>10.3.1向算法传递函数</h4><p>希望将words按照长度排序，长度相同的情况下按字典序排序。为了按长度重排vector,我们将使用sort的第二个版本，此版本是重载过的，它还有第三个参数，这个参数是一个 <strong>谓词</strong> </p><h5 id="谓词"><a href="#谓词" class="headerlink" title="谓词"></a>谓词</h5><p>谓词是一个可调用的表达式，其返回结果是一个能用作条件的值。标准库算法所用的谓词分为两类：一元谓词和二元谓词，分别代表只接受一个参数和两个参数。接受谓词参数的算法对输入序列中的元素调用谓词。因此，元素类型必须能转换为谓词的参数类型。<br>自定义的谓词表达式：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isShorter</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span> &amp; s1,<span class="keyword">const</span> <span class="built_in">string</span> &amp;s2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> s1.size() &lt; s2.size();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//由短到长重新排列</span></span><br><span class="line">sort(words.begin(),words.end(),isShorter);</span><br></pre></td></tr></table></figure></p><h5 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h5><p>当我们将words按长短大小重新排列时，还希望具有相同长度的元素按照字典排序。为了满足上述要求，我们采用stable_sort()，这个稳定排序算法能保证当两个元素相等时不改变它们的相对顺序。<br>新的代码<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elimDup(words);</span><br><span class="line">stable_sort(words.begin(),words.end(),isShorter);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;s : words)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>练习10.13</p><h4 id="10-3-2-lambda表达式"><a href="#10-3-2-lambda表达式" class="headerlink" title="10.3.2 lambda表达式"></a>10.3.2 lambda表达式</h4><p>根据算法接受一元谓词或二元谓词，我们传递给算法的谓词必须严格接受一个或两个参数，但是有时候我们需要传递更多的参数，超出了算法对谓词参数个数的限制。比如在上例中，我们希望输出长度大于等于5的单词的个数，并且只打印这些长度满足条件的单词。</p><p>lambda 表达式:<br>[捕获列表] (参数列表)-&gt; 返回值类型 {函数体}<br>捕获列表中的变量表示要传递到函数体的局部变量，lambda表达式相当于匿名的函数。其也是通过函数调用符调用的。<br>我们可以忽视参数列表和返回值类型，但必须永远包含捕获列表和函数体。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> f  = []&#123;<span class="keyword">return</span> <span class="number">42</span>&#125;;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt;f()&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p><p>lambda表达式的一个简单的用法如上所示。当返回值类型为空时，lambda根据代码的返回值推断返回类型，如果最后一句是return时，返回值类型与要return的值的类型相同，否则类型为空。</p><h5 id="向lambda传递参数"><a href="#向lambda传递参数" class="headerlink" title="向lambda传递参数"></a>向lambda传递参数</h5><p>isShorter的lambda版本<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ ] (<span class="keyword">const</span> <span class="built_in">string</span> &amp;s1,<span class="keyword">const</span> <span class="built_in">string</span> &amp;s2)&#123;<span class="keyword">return</span> s1.size()&lt;s2.size();&#125;</span><br></pre></td></tr></table></figure></p><h5 id="使用捕获列表"><a href="#使用捕获列表" class="headerlink" title="使用捕获列表"></a>使用捕获列表</h5><p>判断一个string类型长度是否大于5<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sz = <span class="number">5</span>;</span><br><span class="line">[sz] (<span class="keyword">const</span> <span class="built_in">string</span>&amp;s)&#123;<span class="keyword">return</span> s.size()&gt;=sz;&#125;</span><br></pre></td></tr></table></figure></p><h5 id="调用find-if-函数"><a href="#调用find-if-函数" class="headerlink" title="调用find_if()函数"></a>调用find_if()函数</h5><p>查找第一个程度大于等于5的字符串<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sz = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">auto</span> wc = find_if(words.begin(),words.end(),[sz] (<span class="keyword">const</span> <span class="built_in">string</span> &amp;s)&#123;<span class="keyword">return</span> s.size() &gt;= sz;&#125;);<span class="comment">//find_if 返回第一个满足条件的迭代器,</span></span><br></pre></td></tr></table></figure></p><h5 id="调用for-each函数"><a href="#调用for-each函数" class="headerlink" title="调用for_each函数"></a>调用for_each函数</h5><p>打印所有长度大于等于5的字符串<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sz = <span class="number">5</span>;</span><br><span class="line">for_each(words.begin(),words.end(),[sz] (<span class="keyword">const</span> <span class="built_in">string</span> &amp;s1)&#123;<span class="keyword">if</span> (s1.size()&gt;=<span class="number">5</span>) <span class="built_in">cout</span> &lt;&lt; s1&lt;&lt;<span class="built_in">endl</span>;&#125;)</span><br><span class="line">或者</span><br><span class="line">for_each(wc,words.end(),[](<span class="keyword">const</span> <span class="built_in">string</span> &amp;s1)&#123;<span class="built_in">cout</span> &lt;&lt; s1&lt;&lt;<span class="built_in">endl</span>;&#125;)</span><br></pre></td></tr></table></figure></p><h4 id="10-3-3-lambda的捕获与返回"><a href="#10-3-3-lambda的捕获与返回" class="headerlink" title="10.3.3 lambda的捕获与返回"></a>10.3.3 lambda的捕获与返回</h4><h5 id="值捕获"><a href="#值捕获" class="headerlink" title="值捕获"></a>值捕获</h5><h5 id="引用捕获"><a href="#引用捕获" class="headerlink" title="引用捕获"></a>引用捕获</h5><h5 id="隐式捕获"><a href="#隐式捕获" class="headerlink" title="隐式捕获"></a>隐式捕获</h5><h5 id="可变lambda"><a href="#可变lambda" class="headerlink" title="可变lambda"></a>可变lambda</h5><p>lambda表达式可以通过mutable关键字改变传入lambda表达式中的参数值，也可以用引用的方式，只要引用不是const类型的就可以。</p><h5 id="指定lambda返回值类型"><a href="#指定lambda返回值类型" class="headerlink" title="指定lambda返回值类型"></a>指定lambda返回值类型</h5><p>当lambda表达式函数体包含return之外的任何语句时，lambda都会推导出返回值类型为void<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">transform(vec.begin(),vec.end(),vec.begin(),[](<span class="keyword">const</span> <span class="keyword">int</span> &amp; i)&#123; <span class="keyword">return</span> i&lt;<span class="number">0</span>?-i:i;&#125;)<span class="comment">//正确</span></span><br><span class="line">transform(vec.begin(),vec.edn(),vec.begin(),[](<span class="keyword">const</span> <span class="keyword">int</span> &amp; i)&#123;<span class="keyword">if</span>(i&lt;<span class="number">0</span>) <span class="keyword">return</span> -i;<span class="keyword">else</span> <span class="keyword">return</span> i;&#125;) <span class="comment">//错误,没有显示指定返回值类型，且有多行语句，lamdba推断出返回值类型为void，与期望值不符</span></span><br></pre></td></tr></table></figure></p><p>当我们需要为一个lambda定义返回值类型时，必须使用尾置返回类型。尾置返回类型跟在形参列表后面并以-&gt;符号开头<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform(vec.begin(),vec.end(),vec.begin(),[](<span class="keyword">int</span> i)-&gt;<span class="keyword">int</span> &#123;<span class="keyword">if</span>(i&lt;<span class="number">0</span>) <span class="keyword">return</span> -i;<span class="keyword">else</span> <span class="keyword">return</span> i;&#125;)</span><br></pre></td></tr></table></figure></p><p>练习 10.20<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; words = &#123; <span class="string">"the"</span>, <span class="string">"quick"</span>, <span class="string">"red"</span>, <span class="string">"fox"</span>, <span class="string">"jumps"</span>, <span class="string">"over"</span>, <span class="string">"the"</span>, <span class="string">"slow"</span>, <span class="string">"red"</span>, <span class="string">"turtle"</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> sz = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">int</span> ret = count_if(words.begin(), words.end(), [=](<span class="keyword">const</span> <span class="built_in">string</span> &amp;s1)&#123;<span class="keyword">return</span> s1.size() &gt;= sz; &#125;);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; ret &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">int</span> ret2 = count_if(words.begin(), words.end(), [=](<span class="keyword">const</span> <span class="built_in">string</span> &amp;s1)-&gt;<span class="keyword">bool</span>&#123; <span class="keyword">if</span> (s1.size() &gt;= sz) <span class="keyword">return</span> <span class="literal">true</span>; <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>; &#125;);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; ret2 &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p><h5 id="标准库bind函数"><a href="#标准库bind函数" class="headerlink" title="标准库bind函数"></a>标准库bind函数</h5><p>待看….</p><h3 id="10-4-再探迭代器"><a href="#10-4-再探迭代器" class="headerlink" title="10.4 再探迭代器"></a>10.4 再探迭代器</h3><p>除了为每个容器定义的迭代器之外，标准库在头文件iterator中还定义了额外几种迭代器。这些迭代器包括以下几种:</p><ol><li>插入迭代器</li><li>流迭代器</li><li>反向迭代器</li><li>移动迭代器</li></ol><h4 id="10-4-1-插入迭代器"><a href="#10-4-1-插入迭代器" class="headerlink" title="10.4.1 插入迭代器"></a>10.4.1 插入迭代器</h4><p>插入器是一种迭代器适配器，它接受一个容器，生成一个迭代器，能实现向给定容器添加元素。当我们通过插入迭代器进行赋值时，该迭代器调用容器操作向给定容器的指定位置插入元素。</p><p>练习 10.26，解释三种插入迭代器的不同之处？</p><ol><li>back_inserter ，调用容器的push_back()方法</li><li>front_inserter, 调用 push_front()方法</li><li>inserter, 调用 c.insert(iter,val)方法，c为迭代器绑定的容器，iter为指定要插入的迭代器位置，val为要插入的值，该方法将val插入到迭代器iter所在位置之前。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">it = inserter(c,iter);</span><br><span class="line">*it = val;</span><br><span class="line"><span class="comment">//等价于</span></span><br><span class="line">it = c.insert(iter,val);</span><br><span class="line">++it;</span><br></pre></td></tr></table></figure><p>练习 10.27<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; words = &#123; <span class="string">"the"</span>, <span class="string">"quick"</span>, <span class="string">"red"</span>, <span class="string">"fox"</span>, <span class="string">"jumps"</span>, <span class="string">"over"</span>, <span class="string">"the"</span>, <span class="string">"slow"</span>, <span class="string">"red"</span>, <span class="string">"turtle"</span> &#125;;</span><br><span class="line"><span class="built_in">list</span>&lt;<span class="built_in">string</span>&gt; lis;</span><br><span class="line">sort(words.begin(), words.end());</span><br><span class="line">unique_copy(words.begin(), words.end(), back_inserter(lis));</span><br></pre></td></tr></table></figure></p><h4 id="10-4-2-iostream-迭代器"><a href="#10-4-2-iostream-迭代器" class="headerlink" title="10.4.2 iostream 迭代器"></a>10.4.2 iostream 迭代器</h4><p>虽然iostream 不是容器，但标准库定义了可以用于这些IO类型对象的迭代器。 istream_iterator 读取输入流，ostream_iterator向一个输出流写数据，这些迭代器将它们对应的流当作一个特定类型的元素序列来处理。通过使用流迭代器，我们可以用泛型算法从流对象读取数据以及向其写入数据。</p><h5 id="istream-iterator"><a href="#istream-iterator" class="headerlink" title="istream_iterator"></a>istream_iterator</h5><p>当创建一个流迭代器时，必须指定迭代器将要读写的对象类型。一个istream_iterator使用&gt;&gt;来读取流。因此，istream_iterator要读取的类型必须定义了输入运算符。当创建一个istream_iterator时，将它绑定到一个输入流上。当然，我们还可以默认初始化迭代器，这样就创建了一个可以当做尾后值使用的迭代器<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; init_iter(<span class="built_in">cin</span>);</span><br><span class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; eof;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line"><span class="keyword">while</span>(init_iter!=eof)</span><br><span class="line">&#123;</span><br><span class="line">    vec.push_back(*init_iter++);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//直到遇见文件结尾或非法字符时终止循环</span></span><br><span class="line"><span class="comment">//将流中的数据赋值给vector的另一个写法是</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;vec(init_iter,eof);</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第十章-泛型算法&quot;&gt;&lt;a href=&quot;#第十章-泛型算法&quot; class=&quot;headerlink&quot; title=&quot;第十章 泛型算法&quot;&gt;&lt;/a&gt;第十章 泛型算法&lt;/h1&gt;&lt;h2 id=&quot;10-1-概述&quot;&gt;&lt;a href=&quot;#10-1-概述&quot; class=&quot;headerlink&quot; title=&quot;10.1. 概述&quot;&gt;&lt;/a&gt;10.1. 概述&lt;/h2&gt;&lt;p&gt;标准库已经为容器类型定义了一些基本操作，但是并没有为各种容器定义诸如排序，查找特定元素等算法，但是标准库定义了一些泛型算法用来解决诸如容器的排序查找等问题。之所以称之为泛型算法是因为，这些算法针对多种容器的多种数据类型均有效。&lt;br&gt;大多数算法都定义在头文件algorithm 中，一般情况下，&lt;strong&gt;泛型算法并不直接操作容器 &lt;/strong&gt;，而是遍历由两个迭代器指定的元素的范围进行操作。&lt;br&gt;迭代器令算法不依赖于容器，但是算法依赖于元素操作的类型，因为算法常常需要比较容器中两个元素的大小，这就要求元素类型支持“&amp;lt;”或”==”运算符，不过大多数算法都可以使用自定义操作来代替运算符。&lt;/p&gt;
    
    </summary>
    
      <category term="C++" scheme="http://matafight.github.io/categories/C/"/>
    
    
  </entry>
  
  <entry>
    <title>随机梯度下降算法简介</title>
    <link href="http://matafight.github.io/2017/01/25/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <id>http://matafight.github.io/2017/01/25/随机梯度下降算法/</id>
    <published>2017-01-25T02:50:52.000Z</published>
    <updated>2017-12-12T13:56:25.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-overview-of-gradient-descent-optimization-method"><a href="#An-overview-of-gradient-descent-optimization-method" class="headerlink" title="An overview of gradient descent optimization method"></a>An overview of gradient descent optimization method</h1><p>梯度下降算法是一种非常流行的优化算法，它是优化神经网络的最常用的算法之一。当前的具有最优性能的深度学习工具包基本上都包含了不同的实现梯度下降的算法，但是，这些算法在使用的时候基本上都是当作一个黑箱子在使用，用户不知道其具体的实现细节，因为，具体地描述这些算法的优点或缺点是比较困难的。</p><p>这篇博客旨在给读者提供一个各个梯度下降算法之所以有用的直觉，从而帮助用户针对具体的问题选择合适的梯度下降算法。我们首先介绍几种梯度下降算法 的变种，然后总结一下使用该算法训练中会出现的一些挑战，之后具体地介绍常见的优化算法以及它们解决上述挑战的动机和相应的更新规则。我们简单地给出了梯度下降算法的平行化和分布式化的一些设置。最终，考虑一些有助于优化梯度下降算法的特别的策略。</p><p>梯度下降主要用来最小化目标函数$J(\theta)$ ，其中参数是$\theta \in R^d$，它通过将参数$\theta$往目标函数相对于参数$\theta$的梯度相反的方向进行更新来一步步地减小目标函数值。 其中学习率$\eta$决定了每次更新的步长。 换句话说，假设目标函数是一座山，参数每次都从山的最陡峭的那一面往下面滚，直到到达谷底，即是局部最小点。</p><a id="more"></a><h2 id="梯度下降的变种"><a href="#梯度下降的变种" class="headerlink" title="梯度下降的变种"></a>梯度下降的变种</h2><p>一共有三种不同的梯度下降形式，其区别在于每次使用多少数据来计算目标函数的梯度。通过使用不同数量的数据，我们在参数更新的准确度与其所消耗的时间之间达到一个平衡。</p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient  descent"></a>Batch gradient  descent</h3><p>BGD ，每次都使用所有的训练数据计算目标函数相对于待求参数的梯度。<br>$$\theta = \theta - \eta.grad(J(\theta))$$<br>因为每次更新都要利用所有训练数据计算梯度，BGD 会非常慢，而且内存可能也不够一次性导入这么多数据。BGD 也不支持在线的更新方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">params_grad = evaluate_gradient(loss_function,data, params)</span><br><span class="line">params = params - learn_rate*params_grad</span><br></pre></td></tr></table></figure><p>其中迭代次数 nb_epochs 是预先定义的，每次迭代过程中，首先计算梯度向量 params_grad，它是所有训练数据的损失函数相对于参数的梯度。现有的深度学习工具包都实现了非常有效的自动求梯度(automatic differentiation)的方法，当然，如果你想自己手写求梯度的代码，梯度检验(gradient checking)是非常有必要的。之后我们将参数向着梯度的方向进行更新，学习率(learn_rate) 决定了每一次更新的步长。对于凸的损失函数，BGD能确保收敛到全局最优解，对于非凸函数，BGD能收敛到局部最优解。</p><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><hr><p>SGD 每次使用单个样本进行参数更新，假设第$i$ 个训练样本的属性为$x_{i}$，标号为$y_i$,那么使用第$i$个样本的更新公式为：<br>$$\theta = \theta - \eta*grad(J(\theta,x_i,y_i))$$<br>BGD 每次迭代都是用相同的样本计算梯度，因此其包含很多冗余的计算.SGD 通过每次只使用一个样本更新来解决这种冗余问题，因此，SGD通常速度非常快，而且其适用于在线学习。<br>值得注意的是，SGD更新得到的参数值波动非常大， 也即更新得到参数的方差非常大。</p><p>对于非凸函数，虽然BGD能够收敛到局部最优解，但是这个局部最优解可能并不是最好的，SGD能够使得参数跳出这个局部最优点，从而获得更好的解。<br>当然，如果我们逐步地减小学习率，对于非凸函数，SGD最终也会像BGD那样收敛到一个局部最优解。对于凸函数，SGD也能收敛到全局最优解。</p><p>下面的代码简单地展示了SGD的学习过程，注意，<strong>每次迭代前都要重现排列一下数据</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">np.random.shuffle(data)</span><br><span class="line"><span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">params_grad  = evaluate_gradient(loss_function,example,params)</span><br><span class="line">params = params - learn_rate * params_grad</span><br></pre></td></tr></table></figure></p><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>MBGD 是介于BGD和SGD之间的一种方法，它每次选择$n$样本更新参数 ：</p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F1.PNG" alt=""></p><p>MBGD可以:<br>1) 减小参数更新的方差，从而使得参数更新能够稳定地收敛。<br>2) 能够使用高度优化的矩阵操作，从而能够使得更新mini-batch 非常有效。<br>一般的mini-batch 大小在50 到256之间，但是这个值因具体应用而不同。<br>MBGD 是神经网络训练中最常用的一种算法，SGD一般也可以指代MBGD。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epoches):</span><br><span class="line">np.random.shuffle(data)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data,batch_size  = <span class="number">50</span>):</span><br><span class="line">params_grad = evaluate_gradient(loss_function,batch,params)</span><br><span class="line">params = params - learn_rate * params_grad</span><br></pre></td></tr></table></figure></p><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><hr><p>上述提到的几种梯度下降算法并不能保证好的收敛效率，它们提出了以下的几个挑战：<br>1）选择一个合适的学习率非常重要，如果学习率设置地非常小的话，收敛率非常慢。但是，学习率过大的话，算法不容易收敛，导致目标函数在局部最小点周围跳转而不收敛到最小点。<br>2）也可以在迭代的过程中动态地改变学习率，但是需要预先定义一些规则。但是这种规则不能适用于不同的数据<br>3) 另一个主要的挑战是，当目标函数是高度非凸的时候，比如神经网络，我们希望避免参数陷在次优点，而且，在这种情况下，参数极易陷入到鞍点，而不是局部最优点。</p><h4 id="Gradient-descent-optimization-algorithms"><a href="#Gradient-descent-optimization-algorithms" class="headerlink" title="Gradient descent optimization algorithms"></a>Gradient descent optimization algorithms</h4><hr><p>接下来我们介绍几种深度学习中经常用到的算法来应对上述挑战，我们不会讨论对高维数据不可行的算法，比如二阶算法，如牛顿法。</p><h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><hr><p>一般的SGD算法在遇到山谷时会发生剧烈震荡，也即在达到局部最优点的过程中非常缓慢，Momentum能够加速收敛到局部最优点，而且能够避免过多的震荡。在每次更新中，它将当前的梯度加上上一次更新的梯度的加权和作为参数更新的步长，具体如下式所示：</p><p><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F2.PNG" alt=""><br>在实际操作中，常常将$\gamma$设为0.9. 可以想象将一个小球滚下山坡，小球会沿着最陡的方向越跑越快，在Momentum中的参数更新中，梯度最大的方向每次都被保留下来传入到下一次参数的更新中，经过多次迭代，最陡峭的方向累积的梯度和越来越大，也即参数向这个方向更新的步长越来越大，因此，能够加速收敛和避免震荡。</p><h4 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h4><hr><p>但是，一个只会一直沿着最陡方向下降的小球是不够智能的，我们希望这个小球能够预知当前选择的方向会不会一直下降，因为可能虽然小球当前所运动地方是下降的，但是未来会上升。<br>相比于 Momentum,Nesterov accelerated gradient （NAG）根据当前的参数值估计下一步参数的梯度，将参数沿着下一步梯度的方向移动<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F3.PNG" alt=""></p><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad 也是一种基于梯度的优化算法，它能够自适应地调整学习率，对于不频繁更新的特征使用较大的学习率，而对于频繁更新的参数使用较小的学习率。所以，Adagrad比较适用于稀疏数据。<br>在上面的算法中，我们使用向量化的方式来更新参数$\theta$,因为Adagrad 对每个不同的参数 $\theta_i$<br>在不同的时刻使用不同的学习率，<br>因此，我们首先对每个单独的参数 $\theta_i$<br> 介绍更新规则，之后再给出向量化的更新公式。假设 $g_{t,i}$<br>  表示目标函数 $J(\theta)$ 在时刻t关于参数 $\theta_i$的梯度：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F4.PNG" alt=""><br>那么SGD算法的更新公式为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F5.PNG" alt=""><br>在Adagrad算法中，$\theta_i$在当前时刻的学习率由上一时刻的梯度计算得到：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F6.PNG" alt=""><br>$G_t \in R^{d\times d}$是对角矩阵，其对角线上的元素$G_{t,ii}$等于$\theta_i$在时刻t之前所有时刻梯度的平方和，$\epsilon$是光滑项以防止分母为零，通常设置为1e-8.比较有趣的是，如果上式中没有平方根，算法的效果会很差。<br>通过使用矩阵-向量 element-wise product, 上式可以直接向量化为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F7.PNG" alt=""><br>Adagrad 一个最大的优点就是不用手动调节学习率，通常将$\eta$设置为0.01,然后就可以自动调节。<br>Adagrad一个最大的缺点就是在上式的分母中，梯度的平方和是一直累加的，可能会累加到无限大，然后每次迭代更新会非常缓慢，最终会停止迭代，可能还没有收敛到局部最优点，下面的算法主要是为了解决这个问题。</p><h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><hr><p>Adadelta 算法是Adagrad算法的扩展，主要是为了解决一直下降的学习率问题。与Adagrad一直累计过去的所有的梯度平方和不同的是，Adadelta设定了一个累积和窗口大小为$\omega$</p><p>Adadelta并没有存储过去$\omega$个时刻的梯度的平方，在时刻t的累计梯度平方和被定义为t-1时刻的累积梯度平方和和当前梯度平方的加权和。<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F8.PNG" alt=""><br>$\gamma$通常也设定为0.9，与Momentum中的$\gamma$相同。</p><p>简单整理一下：<br>对于一般的SGD<br>每次迭代的梯度:<img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F9.PNG" alt=""><br>每次更新公式:<img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F10.PNG" alt=""><br>Adagrad 与 Adadelta仅仅是学习率与SGD不同：<br>对于Adagrad:<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F11.PNG" alt=""><br>替换上式中的$G_t$得：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F122.PNG" alt=""><br>上式中分母可以看作是梯度的Root Mean Square Error，所以可以将其替换为:<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F13.PNG" alt=""></p><p>Adadelta的作者发现上述参数$\theta$的更新公式左右两边单位不统一，$\triangle\theta_t$是参数的更新大小，$g_t$是目标函数关于参数的梯度，它们能相等么？为了实现单位的统一，Adadelta中又定义了新的更新方式，首先定义:<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F14.PNG" alt=""><br>为关于参数更新大小的迭代均值，因此有：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F15.PNG" alt=""><br>因为在第t步迭代中，$E[\triangle\theta^2]_t$是未知的，因此，使用t-1时刻的$E[\triangle\theta^2]_{t-1}$来近似。最终得到Adadelta的更新公式为：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F16.PNG" alt=""></p><h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><hr><p>RMSprop是Geoff Hinton在他的coursera 课程上提出来的一种自适应学习率算法，它是与Adadelta各自独立被提出来的。它与Adadelta的目的一样，都是为了解决Adagrad算法中学习率衰减非常快的问题，它实际上与Adadelta算法中的第一种形式相同:<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F17.PNG" alt=""><br>只不过在RMSprop中$\gamma$被固定地设为0.9,$\eta$通常被设为0.001</p><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><hr><p>Adaptive Moment Estimation (Adam) 算法结合了Momentum，Adadelta 以及 RMSprop的优点，Adam是上述几种算法的集合。<br>首先Adam像Momentum那样计算当前梯度更新也利用了过去时刻的梯度：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F18.PNG" alt=""><br>其次，Adam也像Adadelta那样利用了过去时刻梯度的平方：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F19.PNG" alt=""><br>因为每次更新$m_t,v_t$都有偏差，通过下式克服偏差：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F20.PNG" alt=""><br>最终给出了Adam算法的更新公式，类似于Adadelta和RMSprop:<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/%E5%85%AC%E5%BC%8F21.PNG" alt=""></p><p>参考：<br><a href="http://sebastianruder.com/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">http://sebastianruder.com/optimizing-gradient-descent/index.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;An-overview-of-gradient-descent-optimization-method&quot;&gt;&lt;a href=&quot;#An-overview-of-gradient-descent-optimization-method&quot; class=&quot;headerlink&quot; title=&quot;An overview of gradient descent optimization method&quot;&gt;&lt;/a&gt;An overview of gradient descent optimization method&lt;/h1&gt;&lt;p&gt;梯度下降算法是一种非常流行的优化算法，它是优化神经网络的最常用的算法之一。当前的具有最优性能的深度学习工具包基本上都包含了不同的实现梯度下降的算法，但是，这些算法在使用的时候基本上都是当作一个黑箱子在使用，用户不知道其具体的实现细节，因为，具体地描述这些算法的优点或缺点是比较困难的。&lt;/p&gt;
&lt;p&gt;这篇博客旨在给读者提供一个各个梯度下降算法之所以有用的直觉，从而帮助用户针对具体的问题选择合适的梯度下降算法。我们首先介绍几种梯度下降算法 的变种，然后总结一下使用该算法训练中会出现的一些挑战，之后具体地介绍常见的优化算法以及它们解决上述挑战的动机和相应的更新规则。我们简单地给出了梯度下降算法的平行化和分布式化的一些设置。最终，考虑一些有助于优化梯度下降算法的特别的策略。&lt;/p&gt;
&lt;p&gt;梯度下降主要用来最小化目标函数$J(\theta)$ ，其中参数是$\theta \in R^d$，它通过将参数$\theta$往目标函数相对于参数$\theta$的梯度相反的方向进行更新来一步步地减小目标函数值。 其中学习率$\eta$决定了每次更新的步长。 换句话说，假设目标函数是一座山，参数每次都从山的最陡峭的那一面往下面滚，直到到达谷底，即是局部最小点。&lt;/p&gt;
    
    </summary>
    
      <category term="sgd" scheme="http://matafight.github.io/categories/sgd/"/>
    
    
      <category term="sgd" scheme="http://matafight.github.io/tags/sgd/"/>
    
  </entry>
  
  <entry>
    <title>爬虫入门</title>
    <link href="http://matafight.github.io/2016/09/06/scrapy%E5%85%A5%E9%97%A8/"/>
    <id>http://matafight.github.io/2016/09/06/scrapy入门/</id>
    <published>2016-09-06T08:33:23.000Z</published>
    <updated>2017-12-12T13:56:25.557Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在做试验，实验跑得特别慢，经常一跑就要两三天，没有实验结果也不想写论文，所以，这两天就看了一点爬虫的基础知识。</p><p>主要是跟着这个教程来学：<a href="http://cuiqingcai.com/1052.html" target="_blank" rel="noopener">http://cuiqingcai.com/1052.html</a></p><p>最简单的爬虫只需要会使用urllib2库和正则表达式就可以了，不用去考虑多线程，登陆，验证码等问题。爬虫一般是用来在网络上搜集数据的，我们平时用浏览器向特定网站请求数据时，网站会返回html格式的文档，我们所需要的信息一般都包含在html文档中。<br>所以，爬虫首先需要模拟浏览器的行为向网站请求数据，网站会返回html格式的信息，然后用正则表达式从html中提取出我们需要的信息，这就是最简单的爬虫的所需要实现的功能。</p><a id="more"></a><p>urllib2 库是python中用来进行网络通信的基本库，利用它我们可以与特定网站进行通信和传输数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import urllib2</span><br><span class="line">url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line"><span class="comment">#user_agent 用来伪装浏览器，header中还可以有其他key-value 对</span></span><br><span class="line">user_agent = <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>:user_agent&#125;</span><br><span class="line"><span class="comment">#构建一个请求，包含url 和 header 等相关信息</span></span><br><span class="line">request  = urllib2.Request(url,headers = headers)</span><br><span class="line"><span class="comment">#使用urlopen 执行请求，返回值存在response中</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="comment">#html 内容存在content中</span></span><br><span class="line">content = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment">#接下来从content中利用正则表达式提取自己需要的信息就可以了</span></span><br></pre></td></tr></table></figure><p>上述使用方法在应对不需登陆的网站时是可以的，但是大部分网站是需要登陆的，所以需要输入用户名和密码进行登陆。也可以从文件中读取保存的cookie 进行登陆。实例代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import urllib2</span><br><span class="line">import urllib</span><br><span class="line">import cookielib</span><br><span class="line"></span><br><span class="line"><span class="comment">#将cookie读取到这个opener中，之后可以直接用这个opener去访问需要登录的网站</span></span><br><span class="line"><span class="comment">#也可以将cookie 保存到文件中，之后就可以直接从文件中读取</span></span><br><span class="line">cj = cookielib.CookieJar()</span><br><span class="line">opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))</span><br><span class="line">opener.addheaders.append((<span class="string">'User-Agent'</span>,<span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36'</span>))</span><br><span class="line">opener.addheaders.append((<span class="string">'Referer'</span>,<span class="string">'http://gsmis.nuaa.edu.cn/nuaapyxx/login.aspx'</span>))</span><br><span class="line">values = &#123;</span><br><span class="line"><span class="string">"__VIEWSTATE"</span>:<span class="string">"dDwyMTQxMjc4NDIxOztsPF9jdGwwOkltYWdlQnV0dG9uMTtfY3RsMDpJbWFnZUJ1dHRvbjI7Pj5P2nwDe6vBtb+hohKJ5E+WPJkXzQ=="</span>,</span><br><span class="line"><span class="string">"_ctl0:ImageButton1.x"</span>:<span class="string">"25"</span>,</span><br><span class="line"><span class="string">"_ctl0:ImageButton1.y"</span>:<span class="string">"25"</span>,</span><br><span class="line"><span class="string">"_ctl0:txtusername"</span>:<span class="string">"xxx"</span>,</span><br><span class="line"><span class="string">"_ctl0:txtpassword"</span>:<span class="string">"xxx"</span>&#125;</span><br><span class="line">data = urllib.urlencode(values)</span><br><span class="line">url = <span class="string">'http://gsmis.nuaa.edu.cn/nuaapyxx/login.aspx'</span></span><br><span class="line">resp  =opener.open(url,data)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br></pre></td></tr></table></figure><p>上述代码中以post的方式传递数据，这里的键值对的值需要通过浏览器的开发者模式来查看,其他与服务端通讯的信息也可以查看：<br><img src="http://7xiegr.com1.z0.glb.clouddn.com/login_nuaa.PNG" alt=""></p><p>参考：<a href="https://www.daniweb.com/programming/software-development/threads/296783/can-we-use-python-to-get-pass-password-protection-of-a-aspx-website" target="_blank" rel="noopener">https://www.daniweb.com/programming/software-development/threads/296783/can-we-use-python-to-get-pass-password-protection-of-a-aspx-website</a></p><p><a href="http://cuiqingcai.com/968.html" target="_blank" rel="noopener">http://cuiqingcai.com/968.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在做试验，实验跑得特别慢，经常一跑就要两三天，没有实验结果也不想写论文，所以，这两天就看了一点爬虫的基础知识。&lt;/p&gt;
&lt;p&gt;主要是跟着这个教程来学：&lt;a href=&quot;http://cuiqingcai.com/1052.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://cuiqingcai.com/1052.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最简单的爬虫只需要会使用urllib2库和正则表达式就可以了，不用去考虑多线程，登陆，验证码等问题。爬虫一般是用来在网络上搜集数据的，我们平时用浏览器向特定网站请求数据时，网站会返回html格式的文档，我们所需要的信息一般都包含在html文档中。&lt;br&gt;所以，爬虫首先需要模拟浏览器的行为向网站请求数据，网站会返回html格式的信息，然后用正则表达式从html中提取出我们需要的信息，这就是最简单的爬虫的所需要实现的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://matafight.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="spider" scheme="http://matafight.github.io/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>使用pandas读取和处理数据</title>
    <link href="http://matafight.github.io/2016/08/04/%E4%BD%BF%E7%94%A8pandas%E8%AF%BB%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/"/>
    <id>http://matafight.github.io/2016/08/04/使用pandas读取和处理数据/</id>
    <published>2016-08-04T02:36:47.000Z</published>
    <updated>2017-12-12T13:56:25.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a><strong>1.介绍</strong></h1><p>pandas 是python 中一个常用的开源的进行数据处理的package，它有很多功能，但是我常用的就是使用pandas 来读取和处理数据.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br></pre></td></tr></table></figure></p><p>pandas中数据存储的基本单位是data_frame，不同于numpy 中的array，dataframe 有很多种对数据操作的method。</p><p>读入数据:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(filename,sep=<span class="string">' '</span>,header=None)</span><br></pre></td></tr></table></figure></p><p>read_csv方法有很多参数,参见：<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a><br>常用参数：</p><p>file_path:文件路径和名字<br>sep : 一行的数据之间使用什么符号分割的,一般是空格，csv文件是逗号<br>header: 用来当列名的行号，如果没有传入names参数，则header = 0 ,如果传入了names，则为None<br>names:列名，默认为None<br>如果传入header =None ，则列名为[0,1,..]这样的整数。<br>如果仅仅传入上述参数，行的索引也为从0开始的整数。</p><a id="more"></a><h1 id="2-data-frame-选择数据"><a href="#2-data-frame-选择数据" class="headerlink" title="2.data_frame 选择数据"></a><strong>2.data_frame 选择数据</strong></h1><p>官方文档：</p><p><a href="http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-label" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-label</a><br>Pandas 中选择数据有好几种方法，最简单的分别是:</p><h1 id="Selection-by-label"><a href="#Selection-by-label" class="headerlink" title="Selection by label"></a>Selection by label</h1><h2 id="围绕的方法是df-loc-row-indexer-column-indexer"><a href="#围绕的方法是df-loc-row-indexer-column-indexer" class="headerlink" title="围绕的方法是df.loc[row_indexer,column_indexer]"></a>围绕的方法是df.loc[row_indexer,column_indexer]</h2><p>1.选择行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="string">'a'</span>]</span><br><span class="line">df.loc[<span class="string">'a'</span>:<span class="string">'c'</span>]</span><br><span class="line">df.loc[<span class="string">'a'</span>:]</span><br></pre></td></tr></table></figure></p><p>2.选择列:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:,<span class="string">'col_name'</span>]</span><br></pre></td></tr></table></figure></p><h1 id="Selection-by-position"><a href="#Selection-by-position" class="headerlink" title="Selection by position"></a>Selection by position</h1><h2 id="围绕的方法是df-iloc-row-column"><a href="#围绕的方法是df-iloc-row-column" class="headerlink" title="围绕的方法是df.iloc[row,column]"></a>围绕的方法是df.iloc[row,column]</h2><p>1.选择行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[1:3]</span><br><span class="line">df.iloc[:3]</span><br></pre></td></tr></table></figure></p><p>选择指定行，指定列<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[1:3,2:4]</span><br><span class="line">df.iloc[:,1:3]</span><br></pre></td></tr></table></figure></p><p>2.选择列：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[:,3]</span><br></pre></td></tr></table></figure></p><h1 id="除了用df-loc和df-iloc来选择行和列外，还有其他方法"><a href="#除了用df-loc和df-iloc来选择行和列外，还有其他方法" class="headerlink" title="除了用df.loc和df.iloc来选择行和列外，还有其他方法"></a>除了用df.loc和df.iloc来选择行和列外，还有其他方法</h1><h2 id="使用Attribute-access-来选择列："><a href="#使用Attribute-access-来选择列：" class="headerlink" title="使用Attribute access 来选择列："></a>使用Attribute access 来选择列：</h2><p>前提是知道列名:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.a</span><br><span class="line">df[a]</span><br></pre></td></tr></table></figure></p><p>上述两种方法的区别是df[a]可以新增一列，列名为a,df.a只能读取</p><h2 id="使用-operator来选择行："><a href="#使用-operator来选择行：" class="headerlink" title="使用[]operator来选择行："></a>使用[]operator来选择行：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[:5]</span><br><span class="line">选择前五行</span><br></pre></td></tr></table></figure><h1 id="data-frame其他相关用法"><a href="#data-frame其他相关用法" class="headerlink" title="data_frame其他相关用法"></a><strong>data_frame其他相关用法</strong></h1><p>df.values<br>返回一个多维数组</p><p>df.drop(“row_label”)<br>df.drop(“column_label”,axis=1)<br>分别删除行和列</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-介绍&quot;&gt;&lt;a href=&quot;#1-介绍&quot; class=&quot;headerlink&quot; title=&quot;1.介绍&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.介绍&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;pandas 是python 中一个常用的开源的进行数据处理的package，它有很多功能，但是我常用的就是使用pandas 来读取和处理数据.&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;import pandas as pd&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;pandas中数据存储的基本单位是data_frame，不同于numpy 中的array，dataframe 有很多种对数据操作的method。&lt;/p&gt;
&lt;p&gt;读入数据:&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;df = pd.read_csv(filename,sep=&lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;,header=None)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;read_csv方法有很多参数,参见：&lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html&lt;/a&gt;&lt;br&gt;常用参数：&lt;/p&gt;
&lt;p&gt;file_path:文件路径和名字&lt;br&gt;sep : 一行的数据之间使用什么符号分割的,一般是空格，csv文件是逗号&lt;br&gt;header: 用来当列名的行号，如果没有传入names参数，则header = 0 ,如果传入了names，则为None&lt;br&gt;names:列名，默认为None&lt;br&gt;如果传入header =None ，则列名为[0,1,..]这样的整数。&lt;br&gt;如果仅仅传入上述参数，行的索引也为从0开始的整数。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://matafight.github.io/categories/machine-learning/"/>
    
    
      <category term="pandas" scheme="http://matafight.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow 安装及使用</title>
    <link href="http://matafight.github.io/2016/08/03/tenforflow/"/>
    <id>http://matafight.github.io/2016/08/03/tenforflow/</id>
    <published>2016-08-03T05:54:57.000Z</published>
    <updated>2017-12-12T13:56:25.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在ubuntu下安装tensorflow和使用tensorflow"><a href="#在ubuntu下安装tensorflow和使用tensorflow" class="headerlink" title="在ubuntu下安装tensorflow和使用tensorflow"></a><strong>在ubuntu下安装tensorflow和使用tensorflow</strong></h1><h2 id="1-安装cuda-cudnn"><a href="#1-安装cuda-cudnn" class="headerlink" title="1.安装cuda,cudnn"></a>1.安装cuda,cudnn</h2><p><em>a)</em><br>ubuntu下安装需要先安装g++<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential</span><br></pre></td></tr></table></figure></p><p><em>b)</em>从navida 官网下载cuda的deb文件<br>从命令行安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i cuda-repo-***.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br></pre></td></tr></table></figure></p><p>之后需要在.bashrc 中设置cuda的环境变量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">"<span class="variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda-7.5/lib64"</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/<span class="built_in">local</span>/cuda-7.5</span><br></pre></td></tr></table></figure></p><p>注意上述的命令中需要将cuda的相应版本改成自己下载的版本</p><p><em>c)</em><br>安装cudnn-v4<br>cudnn 的下载需要navidia的开发者账号，去注册一下就行，我的账号就是edu账号。<br>cudnn的具体安装方式参照的是tensorflow给出的安装指南</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Uncompress and copy the cuDNN files into the toolkit directory. Assuming the toolkit is installed <span class="keyword">in</span> /usr/<span class="built_in">local</span>/cuda, run the following commands (edited to reflect the cuDNN version you downloaded):</span><br><span class="line"></span><br><span class="line">tar xvzf cudnn-7.5-linux-x64-v4.tgz</span><br><span class="line">sudo cp cudnn-7.5-linux-x64-v4/cudnn.h /usr/<span class="built_in">local</span>/cuda/include</span><br><span class="line">sudo cp cudnn-7.5-linux-x64-v4/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64</span><br><span class="line">sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>注意上面的cudnn的路径有点不正确，需要根据具体的文件路径和相应的版本更改命令。</p><p><em>d)</em>关闭与nvidia 不兼容的nouveau<br>打开Terminal终端（Ctrl+Alt+T）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo edit /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure></p><p>在文件中写入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"><span class="built_in">alias</span> nouveau off</span><br><span class="line"><span class="built_in">alias</span> lbm-nouveau off</span><br></pre></td></tr></table></figure></p><p>在terminal中执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf</span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></p><p><em>e)</em><br>验证cuda的安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line">sudo make</span><br><span class="line">sudo ./deviceQuery</span><br><span class="line">result ：pass 则为安装成功。</span><br></pre></td></tr></table></figure></p><h2 id="2-安装tensorflow"><a href="#2-安装tensorflow" class="headerlink" title="2.安装tensorflow"></a>2.安装tensorflow</h2><p>使用官网的Anaconda的安装方式<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda create -n tensorflow python=2.7</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> activate tensorflow</span><br><span class="line"></span><br><span class="line">(tensorflow)$ <span class="built_in">export</span> TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">(tensorflow)$ pip install --upgrade <span class="variable">$TF_BINARY_URL</span></span><br></pre></td></tr></table></figure></p><p>测试安装：参考官网</p><p>使用anaconda安装方式的特点是，在Anaconda中独立出来一个环境，名字叫做tensorflow，每次要使用tensorflow的时候，先切换到这个环境，然后再这个环境中安装和运行相关程序。环境里面的python 与 anaconda中的python是相互独立的。</p><p>注意这里提供的安装方法与可能与个人的电脑相关设置有所不同，仅供参考</p><p>参考:<a href="http://blog.csdn.net/songrotek/article/details/50770154" target="_blank" rel="noopener">http://blog.csdn.net/songrotek/article/details/50770154</a></p><h2 id="3-tensorflow-相关用法"><a href="#3-tensorflow-相关用法" class="headerlink" title="3.tensorflow 相关用法"></a>3.tensorflow 相关用法</h2><p><a href="https://github.com/Matafight/TFstudyNotes" target="_blank" rel="noopener">https://github.com/Matafight/TFstudyNotes</a></p><p>to be continue…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;在ubuntu下安装tensorflow和使用tensorflow&quot;&gt;&lt;a href=&quot;#在ubuntu下安装tensorflow和使用tensorflow&quot; class=&quot;headerlink&quot; title=&quot;在ubuntu下安装tensorflow和使用ten
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://matafight.github.io/categories/tensorflow/"/>
    
    
      <category term="deep learning" scheme="http://matafight.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
