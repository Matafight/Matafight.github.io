<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="this is amazing"><title>Attention is all you need | Matafight's world</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Attention is all you need</h1><a id="logo" href="/.">Matafight's world</a><p class="description">点滴进步</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Attention is all you need</h1><div class="post-meta">Jul 13, 2019<span> | </span><span class="category"><a href="/categories/machine-learning/">machine learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2019/07/13/Attention-is-all-you-need/" href="/2019/07/13/Attention-is-all-you-need/#comments" class="ds-thread-count"></a><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>###encoder-decoder框架</p>
<p>在nlp任务中，比如机器翻译，encoder-decoder是一种比较常用的学习框架，将原文x输入到encoder中，encoder输出context c作为decoder的部分输入，这里的c就包含了原文x的所有信息。</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN由于擅长处理序列特征，因此常被用在nlp任务中，经典的encoder-decoder框架就是基于RNN搭建而成。RNN的缺点在于其无法很好地并行化，这是由其工作原理决定的，当前时刻的输入依赖于上一时刻的输出，因此，其只能串行地运行</p>
<p><img src="https://wx1.sinaimg.cn/large/6b6c6d13ly1g49qx13slsj214o0ewtid.jpg" alt=""></p>
<h4 id="RNN-Attention"><a href="#RNN-Attention" class="headerlink" title="RNN+Attention"></a>RNN+Attention</h4><p>在上图中可以看到，encoder只输出一个context，decoder中的所有单词都基于相同的context，但是在机器翻译中这一点不符合直觉，因为翻译和原文中的单词通常是有对应关系的，所以对于每个decoder中的输入的hidden state都需要找到其对encoder中最感兴趣的hidden state。RNN存在长程梯度消失问题，对于较长的句子，很难希望通过将输入的序列转化为定长的向量来保存所有信息，Attention机制也能克服这个问题。</p>
<p>具体算法：</p>
<p>假设encoder 的每个hidden state 是 key,同时hidden state也是value, 则encoder会产生一个键值对，decoder的hidden state是query，那么在attention机制中，计算query的attention的方法就是：先计算query与encoder中所有key的相似性，把这个相似性作为权重，得到 各个key-value对的加权和作为query的attention输出。</p>
<p><img src="https://ws4.sinaimg.cn/large/6b6c6d13ly1g49r038ezwj213s0nqh83.jpg" alt=""></p>
<h4 id="RNN-Attention-Self-attention"><a href="#RNN-Attention-Self-attention" class="headerlink" title="RNN+Attention+Self attention"></a>RNN+Attention+Self attention</h4><p>Attention 是encoder和decoder之间的机制，而self attention 则是encoder或者decoder内部的机制。因为一句话是有上下文的，一句话中的单词可能依赖于这句话中的其他单词，比如：”我喜欢吃火锅，不喜欢吃醋“ 这句话中”喜欢“对“火锅“应该有比较高的关注度，”不喜欢“对“醋”有比较高的关注度。具体实现方式与attention类似，只需要在encoder或者decoder内部加一个或多个attention layer就可以实现self attention。</p>
<p><img src="https://wx4.sinaimg.cn/large/6b6c6d13ly1g49r0j2n15j20wi0swn92.jpg" alt=""></p>
<p>####Transformer</p>
<p>前面介绍的都是基于RNN的encoder-decoder模型，但是RNN本身的问题是无法并行化，Transformer 也有encoder和decoder两部分，它完全抛弃了RNN的结构，只用了Attention和self-attention，因此，Transformer相比之前基于RNN的模型更易并行化。</p>
<p><img src="//wx3.sinaimg.cn/large/6b6c6d13gy1g4v41ul814j20fm0nn77t.jpg" alt=""></p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="Encoder-部分"><a href="#Encoder-部分" class="headerlink" title="Encoder 部分"></a>Encoder 部分</h3><p>输入一个sentence的embedding，先输入到一个self attention层，进行layer-norm之后进入到后面的 fully connected feed forward network中，encoder的output就是ffn 的输出加上layer norm之后的结果。 </p>
<h3 id="Decoder-部分"><a href="#Decoder-部分" class="headerlink" title="Decoder 部分"></a>Decoder 部分</h3><p>与encoder的区别在于decoder中会有来自encoder的cross-attention，并且decoder中attention信息的流动是单向的，decoder的input在self attention时只能“看到”其前部的信息，而“看不到”后面的信息，Transformer的这种结构使得其属于  <strong>language generation</strong> 模型。</p>
<h4 id="Scaled-dot-product-attention"><a href="#Scaled-dot-product-attention" class="headerlink" title="Scaled-dot-product attention"></a>Scaled-dot-product attention</h4><p>attention 中最核心的是三个 数组：key，query和value， scaled-dot-product attention 就是用内积衡量query和key之间的相似性，并将其作为key对应value的权重。</p>
<p><img src="//ws1.sinaimg.cn/large/6b6c6d13gy1g4v41erh2hj20dv02wjrd.jpg" alt=""></p>
<h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><p>论文提出了一种新的attention机制： Multi-head attention。它基于 Scaled-dot-product attention 而来， 它先将 key,query和value投影到n个指定大小的子空间中，然后在各个子空间分别做 scaled-dot-product attention，最后将n个子空间的attention的输出concat 在一起，然后最后经过一个线性变换将输出投影到指定大小的空间中。</p>
<h4 id="Fully-connected-feed-forward-network"><a href="#Fully-connected-feed-forward-network" class="headerlink" title="Fully connected feed-forward network"></a>Fully connected feed-forward network</h4><p>全连接前馈神经网络，论文中用了文本领域常用的一维卷积来实现</p>
<p><img src="//ws1.sinaimg.cn/large/6b6c6d13gy1g4v40p53ygj213q17679a.jpg" alt=""></p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于encoder的input中的token会关注所有其他token，而且到现在为止整个模型并没有考虑输入的顺序，所以将输入随便排序都不会影响结果，为了考虑序列顺序信息，论文将每个位置的input都加上其对应的positional encoding，其计算公式如下：</p>
<p><img src="//ws3.sinaimg.cn/large/6b6c6d13gy1g4v4gri6vkj20a9029glo.jpg" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文主要在machine translation数据集上进行了实验，包括了WMT 2014 English-to-German translation task  和 WMT 2014 English-to-French translation task 。结果当然也证明了该方法的有效性了。</p>
<p><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matafight.github.io/2019/07/13/Attention-is-all-you-need/" data-id="cjy1l4ual0002149waqelp0iu" class="article-share-link">分享到</a><div class="tags"><a href="/tags/attention/">attention</a></div><div class="post-nav"><a href="/2019/06/21/理解word2vec/" class="next">理解word2vec</a></div><div data-thread-key="2019/07/13/Attention-is-all-you-need/" data-title="Attention is all you need" data-url="http://matafight.github.io/2019/07/13/Attention-is-all-you-need/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2019/07/13/Attention-is-all-you-need/" data-title="Attention is all you need" data-url="http://matafight.github.io/2019/07/13/Attention-is-all-you-need/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://matafight.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sgd/">sgd</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim/">vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客本身/">博客本身</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记-Matlab/">杂记|Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知乎/">知乎</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/attention/" style="font-size: 15px;">attention</a> <a href="/tags/毕设/" style="font-size: 15px;">毕设</a> <a href="/tags/XGBoost/" style="font-size: 15px;">XGBoost</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a> <a href="/tags/Model-Selection/" style="font-size: 15px;">Model-Selection</a> <a href="/tags/Cython/" style="font-size: 15px;">Cython</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/kaggle/" style="font-size: 15px;">kaggle</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/latex/" style="font-size: 15px;">latex</a> <a href="/tags/lightgbm/" style="font-size: 15px;">lightgbm</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/normalization/" style="font-size: 15px;">normalization</a> <a href="/tags/编码/" style="font-size: 15px;">编码</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/工具/" style="font-size: 15px;">工具</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/FAQ/" style="font-size: 15px;">FAQ</a> <a href="/tags/machien-learning/" style="font-size: 15px;">machien learning</a> <a href="/tags/好的博客/" style="font-size: 15px;">好的博客</a> <a href="/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/求职/" style="font-size: 15px;">求职</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/tags/sgd/" style="font-size: 15px;">sgd</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/07/13/Attention-is-all-you-need/">Attention is all you need</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/21/理解word2vec/">理解word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/lightgbm-相关参数/">lightgbm 相关参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/12/git-note/">git note</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/Deep-Learning-Chapter16/">Deep Learning Chapter16</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/CNN结构的发展/">CNN结构的发展</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/25/Deep-learning-chapter10-part-1/">Deep learning chapter10</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/17/广义线性模型/">广义线性模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/29/gbdt-xgboost与lightgbm/">gbdt,xgboost与lightgbm</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/kaggle竞赛总结/">kaggle竞赛总结</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://tsien.github.io/" title="tsien" target="_blank">tsien</a><ul></ul><a href="http://www.ruanyifeng.com/blog/" title="阮一峰的网络日志" target="_blank">阮一峰的网络日志</a><ul></ul><a href="http://blog.csdn.net/abcjennifer/article/category/1226975" title="Rachel Zhang的专栏" target="_blank">Rachel Zhang的专栏</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matafight's world.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'matafight1994'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>