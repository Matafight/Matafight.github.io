<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="this is amazing"><title>理解word2vec | Matafight's world</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">理解word2vec</h1><a id="logo" href="/.">Matafight's world</a><p class="description">点滴进步</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">理解word2vec</h1><div class="post-meta">Jun 21, 2019<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2019/06/21/理解word2vec/" href="/2019/06/21/理解word2vec/#comments" class="ds-thread-count"></a><div class="post-content"><p>自然语言处理任务中要处理的对象是单词或者词组，单词可以看做是类别型特征，虽然tree-based模型可以采用类别特征，但包括神经网络在内的大部分机器学习模型只能处理数值型特征。因此，在使用模型时通常需要将单词等特征转化为数值。最常见的方法是one-hot encoding。但这种方法编码出来的特征非常稀疏，不利于特征学习，而且无法度量单词之间的相似度。google在13年提出的word2vec方法利用一个浅层的神经网络将稀疏的特征向量映射到稠密的低维空间中，其动机是：位于相同上下文的单词应当有相似的语义，利用的是单词的共现性。 准确的说，学习出单词的embedding只是模型的一个“副作用”，word2vec是一个框架，它包含两个建模函数，或者说是两个建模角度,分别是CBOW和Skip-Grams。</p>
<h2 id="基本原理："><a href="#基本原理：" class="headerlink" title="基本原理："></a>基本原理：</h2><h3 id="CBOW-通过上下文预测中间的单词"><a href="#CBOW-通过上下文预测中间的单词" class="headerlink" title="CBOW:通过上下文预测中间的单词"></a>CBOW:通过上下文预测中间的单词</h3><p>最大化似然函数<br>$$LL = \prod_{t=1}^Tp(w_t|context(w_t))$$<br>这个$context(w_t)$可以有多种方式处理。</p>
<h3 id="Skip-Grams-通过中间单词预测上下文"><a href="#Skip-Grams-通过中间单词预测上下文" class="headerlink" title="Skip-Grams: 通过中间单词预测上下文"></a>Skip-Grams: 通过中间单词预测上下文</h3><p>目标是最大化似然函数<br>$$LL = \prod<em>{i=1}^T\prod</em>{-c\le j\le c}p(w_{t+j}|w_t)$$<br>其中<br>$$p(w_o|w_i)=\frac{exp(v^{‘T}_ov<em>i)}{\sum</em>{c=1}^{D}exp(v^{‘T}_cv_i)}$$<br>对每个单词都对应两个向量$v^{‘}和v$。</p>
<h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p><img src="https://ws1.sinaimg.cn/large/6b6c6d13ly1g210b935ovj20pe0rmmya.jpg" alt="cbow"><br>基本原理如图所示，假设词典中共有$D$个单词，给定一句话的上下文有$c$个单词，所以需要先对每个单词做one-hot 编码，$x_i$实际上是一个大小为$D\times 1$的向量，矩阵$W<em>1\in R^{D\times e}$ 每一行都可以看做是一个单词的embedding，这是模型的参数，也是我们想得到的单词embedding。 中间的隐层<br> $$ h =\frac{1}{c}\sum</em>{i=1}^c x_i^{T}W_1,h\in R^{1\times e} $$<br>  系数矩阵$W_2 \in R^{e\times D}$ 将隐层的数据重新映射到整个字典空间中.<br>$$o = softmax(h\times W<em>2), o \in R^{1\times D},o</em>{i}\in [0,1]$$<br>输出概率最大的单词作为候选。这里的$W_1,W_2$分别对应上面公式中的$v^{‘},v$.</p>
<h3 id="Skip-Grams"><a href="#Skip-Grams" class="headerlink" title="Skip-Grams"></a>Skip-Grams</h3><p><img src="https://wx3.sinaimg.cn/large/6b6c6d13ly1g210b98vb1j20su0p4myb.jpg" alt="skip_gram"><br>Skip-Grams正好与CBOW的流程相反，其输入层$I$就是一个单词的one-hot编码，矩阵$W_1 \in R^{D\times e}$,<br>$$h = x_i\times W_1, h\in R^{1\times e}$$<br>输出层对应多个单词，对应每个单词都有一个softmax损失。将所有$c$个softmax损失相加就是总的损失。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>由于单词的空间$D$非常庞大，在计算softmax损失的时候需要对所有$D$个单词进行累加，这一方面非常耗时，另一方面会导致权重更新效率低下，因为每次只需要更新$c$个单词对应的权重，但却需要遍历所有单词。</p>
<h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>统计所有样本出现的频次，建立哈夫曼树，树的深度为$logD$,每个叶子节点都对应一个单词$w$,设$n(w,i)$表示从根节点出发能到达叶子节点$w$的路径上的第$i$个节点，并且$n(w,1)=root$,每个内部节点(包括根节点)都可以看做是神经网络中的一个二类分类节点，节点系数为$v<em>{n(w,i)}$.<br>以CBOW举例，获得隐节点向量$h$之后，就将$h$输入到哈夫曼树的根节点,在每个节点执行二类分类任务。$\sigma(x)=\frac{1}{1+e^{-x}}$,当$\sigma(h*v</em>{n(w,i)})&gt;0$时，就传递到左子树，反之，传递到右子树，直到叶子节点，叶子节点所在的单词就是我们要预测的值。</p>
<p>所以实际上层次softmax最多只需要计算$logD$次，每次都是一个二类分类问题。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>$$p(w_o|w_i)=\frac{exp(v^{‘T}_ov<em>i)}{\sum</em>{c=1}^{D}exp(v^{‘T}_cv_i)}$$<br>上式的分母是所有单词，实际上我们可以对单词分类，将我们当前关注的单词$W_i$对应的上下问认为是正类，将所有其他单词认为是负类，在分类的时候只需要区分当前单词是正类还是负类就行了，不需要具体计算每一个单词的概率。所以其实上式可以简化成:</p>
<p>$$p(w_0|w_i) = \sigma(v^{‘T}_ov_i)=\frac{1}{1+e^{-v^{‘T}_ov_i}}(当w_0属于正类) \<br>p(w_0|w_i) = 1-\sigma(v^{‘T}_ov_i)=1-\frac{1}{1+e^{-v^{‘T}_ov_i}}=\sigma(-v^{‘T}_ov_i)(当w_0属于负类)$$<br>这里的正类和负类只是相对于$w_i$来说的。</p>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p>如果采用softmax版本，对每个样本的最大似然函数:<br>$$ln(p(w_o|w_i))=v^{‘T}_ov<em>i-ln(\sum</em>{c=1}^{D}exp(v^{‘T}_cv_i))$$</p>
<p>如果采用负采样的二分类版本，在更新某个样本时的损失函数为:<br>$$ln(p(w_o|w_i))=ln\sigma(v^{‘T}_ov<em>i)+\sum</em>{k}^KE_{w\sim P_n(w)}ln\sigma(-v^{‘T}_ov_i)$$</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matafight.github.io/2019/06/21/理解word2vec/" data-id="cjx65kpnn0038l89wenotd3vw" class="article-share-link">分享到</a><div class="tags"><a href="/tags/word2vec/">word2vec</a></div><div class="post-nav"><a href="/2019/03/24/lightgbm-相关参数/" class="next">lightgbm 相关参数</a></div><div data-thread-key="2019/06/21/理解word2vec/" data-title="理解word2vec" data-url="http://matafight.github.io/2019/06/21/理解word2vec/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2019/06/21/理解word2vec/" data-title="理解word2vec" data-url="http://matafight.github.io/2019/06/21/理解word2vec/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://matafight.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sgd/">sgd</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim/">vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客本身/">博客本身</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记-Matlab/">杂记|Matlab</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/知乎/">知乎</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/XGBoost/" style="font-size: 15px;">XGBoost</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a> <a href="/tags/Cython/" style="font-size: 15px;">Cython</a> <a href="/tags/Model-Selection/" style="font-size: 15px;">Model-Selection</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/kaggle/" style="font-size: 15px;">kaggle</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/lightgbm/" style="font-size: 15px;">lightgbm</a> <a href="/tags/latex/" style="font-size: 15px;">latex</a> <a href="/tags/normalization/" style="font-size: 15px;">normalization</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/编码/" style="font-size: 15px;">编码</a> <a href="/tags/毕设/" style="font-size: 15px;">毕设</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/工具/" style="font-size: 15px;">工具</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/FAQ/" style="font-size: 15px;">FAQ</a> <a href="/tags/求职/" style="font-size: 15px;">求职</a> <a href="/tags/好的博客/" style="font-size: 15px;">好的博客</a> <a href="/tags/machien-learning/" style="font-size: 15px;">machien learning</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/sgd/" style="font-size: 15px;">sgd</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/21/理解word2vec/">理解word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/lightgbm-相关参数/">lightgbm 相关参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/12/git-note/">git note</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/Deep-Learning-Chapter16/">Deep Learning Chapter16</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/CNN结构的发展/">CNN结构的发展</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/25/Deep-learning-chapter10-part-1/">Deep learning chapter10</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/17/广义线性模型/">广义线性模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/29/gbdt-xgboost与lightgbm/">gbdt,xgboost与lightgbm</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/kaggle竞赛总结/">kaggle竞赛总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/python中的编码问题/">python中的编码问题</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://tsien.github.io/" title="tsien" target="_blank">tsien</a><ul></ul><a href="http://www.ruanyifeng.com/blog/" title="阮一峰的网络日志" target="_blank">阮一峰的网络日志</a><ul></ul><a href="http://blog.csdn.net/abcjennifer/article/category/1226975" title="Rachel Zhang的专栏" target="_blank">Rachel Zhang的专栏</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matafight's world.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'matafight1994'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>